\chapter{Fundamentação Teórica}

\section{A Redação no ENEM}

A prova de redação no ENEM desempenha um papel fundamental na qualificação das habilidades dos estudantes, medindo a capacidade de expressarem ideias de maneira clara, coerente e persuasiva. Para avaliar as redações, são estabelecidos critérios objetivos que consideram a estrutura do texto e métricas específicas do exame, baseadas em uma matriz de referência.

\subsection{Estrutura do Texto}

A estrutura da redação no ENEM segue o formato dissertativo-argumentativo, exigindo que os participantes defendam um ponto de vista sobre um tema previamente estabelecido, conforme abordado em \cite{cartilha-redacao}. A composição textual deve respeitar a norma padrão da língua portuguesa e o gênero determinado, mantendo uma introdução, desenvolvimento e conclusão. A clareza na exposição de ideias, a adequação ao tema e a coesão textual são elementos essenciais para uma boa pontuação.

Para os autores \citet{platao-e-fiorin}, a dissertação deve explicitar uma visão concreta da realidade, recorrendo a referências para ilustrar afirmações ou para sustentar argumentos. No contexto do ENEM, esse é o papel exercido pela coesão e articulação da redação, já que o texto deve manter uma progressão lógica e um encadeamento de ideias com o fim de defender o ponto de vista do aluno. Além disso, os autores defendem que a dissertação sempre deve seguir um eixo temático à medida que aborda conceitos do mundo de modo abstrato, sem relações temporais ou espaciais. Nesse sentido, na prova, é imperativo que o texto apresente uma coerência temática, alinhando-se ao tópico proposto.

Ainda ao longo do texto, os estudantes devem articular informações externas para embasar seus discursos, o que é definido, em \cite{platao-e-fiorin}, como argumentos de autoridade e argumentos baseados em provas concretas. Assim, aliado à seleção de elementos coesivos adequados, o propósito da redação deve ser o de convencer o interlocutor em torno do tema abordado.

Por fim, é exigido que os alunos apresentem uma proposta de intervenção na conclusão do texto, que deve conter as ações necessárias para mitigar o problema levantado, os agentes responsáveis, os meios de solução, os possíveis desdobramentos da aplicação da proposta e, por fim, um detalhamento mais refinado da mediação (\cite[p.~20]{cartilha-redacao}).

\subsection{Métricas de Avaliação}

A avaliação das redações no ENEM adota uma abordagem multifacetada, incorporando diversas métricas para assegurar uma análise abrangente, incluindo desde a coesão textual até a capacidade de argumentação e a adequação ao padrão culto da língua. A seleção e organização eficazes de informações também são critérios fundamentais. A combinação desses parâmetros contribui significativamente para a construção de uma avaliação equitativa e holística das habilidades de escrita dos participantes.

Cada redação no ENEM é avaliada com base na matriz de referência do exame, composta por cinco competências, delineadas em suas respectivas habilidades, conforme detalhado em \cite{cartilha-redacao}. A primeira competência concentra-se na avaliação do emprego correto da língua portuguesa, abrangendo aspectos como ortografia, pontuação, concordância, regência verbal, etc. A segunda competência analisa a capacidade de interpretação do participante em relação ao tema proposto. Na terceira competência, são avaliados critérios como seleção e organização de informações, articulação de ideias e construção de argumentos. A quarta competência incide sobre o encadeamento dos argumentos e a progressão temática do texto. Por fim, a quinta competência avalia a habilidade do participante em elaborar uma proposta de intervenção para o problema abordado, considerando a adequação ao tema, a viabilidade prática de aplicação e o respeito aos direitos humanos. As cinco competências podem ser sintetizadas, de acordo com a Cartilha de Redação do ENEM (\cite{cartilha-redacao}), em:

\begin{enumerate}
    \item[\textbf{I.}] Demonstrar domínio da modalidade escrita formal da língua portuguesa.
    \item[\textbf{II.}] Compreender a proposta de redação e aplicar conceitos das várias áreas de conhecimento para desenvolver o tema, dentro dos limites estruturais do texto dissertativo-argumentativo em prosa.
    \item[\textbf{III.}] Selecionar, relacionar, organizar e interpretar informações, fatos, opiniões e argumentos em defesa de um ponto de vista.
    \item[\textbf{IV.}] Demonstrar conhecimento dos mecanismos linguísticos necessários para a construção da argumentação.
    \item[\textbf{V.}] Elaborar proposta de intervenção para o problema abordado, respeitando os direitos humanos.
\end{enumerate}

\subsection{Atribuição de Notas}

A atribuição de notas na redação do ENEM segue um processo rigoroso. Cada uma das cinco competências é avaliada em uma escala de 0 a 200 pontos, variando, nesse intervalo, em valores múltiplos de 40. As redações são corrigidas, inicialmente, por dois especialistas independentes e todo o processo é meticulosamente estruturado para prevenir possíveis divergências nas pontuações finais.

Duas avaliações são consideradas divergentes caso a nota atribuida a qualquer uma das competências difira em mais de 80 pontos ou se a diferença total entre as notas seja superior a 100 pontos. Nesses casos, uma terceira correção é realizada, e a nota final é a média aritmética das duas avaliações mais próximas. Caso todas as avaliações ainda sejam discrepantes entre si, a redação é submetida a uma banca independente, responsável por atribuir a nota final ao texto.

As notas levam em consideração as métricas da matriz de referência do exame, conforme estabelecido em \cite[p.~9-22]{cartilha-redacao}. A pontuação final varia de 0 a 1000 pontos, sendo calculada a partir da soma dos pontos atribuídos a cada competência. Este sistema de avaliação visa garantir uma análise abrangente e justa das redações dos participantes.

A tabela \ref{tab:competencia-1} abaixo, extraída da Cartilha de Redação do ENEM, ilustra a associação entre as pontuações atribuídas à competência I e os níveis de desempenho esperados dos alunos. As demais competências possuem uma associação similar de acordo com seus respectivos campos de avaliação.

\begin{table}[H]
    \centering
    \caption{Níveis de desempenho esperados para a competência I e notas associadas. Tabela extraída de \cite[p.~10]{cartilha-redacao}}
    \label{tab:competencia-1}
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Pontuação} & \textbf{Níveis de desempenho} \\
        \hline
        200 pontos & Demonstra excelente domínio da modalidade escrita formal da língua portuguesa e de escolha de registro. Desvios gramaticais ou de convenções da escrita serão aceitos somente como excepcionalidade e quando não caracterizarem reincidência. \\
        \hline
        160 pontos & Demonstra bom domínio da modalidade escrita formal da língua portuguesa e de escolha de registro, com poucos desvios gramaticais e de convenções da escrita. \\
        \hline
        120 pontos & Demonstra domínio mediano da modalidade escrita formal da língua portuguesa e de escolha de registro, com alguns desvios gramaticais e de convenções da escrita. \\
        \hline
        80 pontos & Demonstra domínio insuficiente da modalidade escrita formal da língua portuguesa, com muitos desvios gramaticais, de escolha de registro e de convenções da escrita. \\
        \hline
        40 pontos & Demonstra domínio precário da modalidade escrita formal da língua portuguesa, de forma sistemática, com diversificados e frequentes desvios gramaticais, de escolha de registro e de convenções da escrita. \\
        \hline
        0 pontos & Demonstra desconhecimento da modalidade escrita formal da língua portuguesa. \\
        \hline
    \end{tabularx}
\end{table}

\section{Avaliação Automática de Redações}

A avaliação automática de redações (AES) tem se destacado significativamente nos últimos anos devido à disparidade entre a disponibilidade de profissionais capacitados para correção de textos e o número expressivo de estudantes. O avanço das técnicas de modelagem de linguagem e processamento de linguagem natural (PLN) impulsionou inúmeros trabalhos na tentativa de simplificar o processo de correção.

Este campo de pesquisa é intrinsecamente multidisciplinar, demandando conhecimentos em linguística, PLN, aprendizado de máquina e estatística. A meta central consiste em desenvolver modelos capazes de atribuir notas automaticamente a redações, seguindo critérios similares aos adotados por especialistas.

No cenário brasileiro, embora as pesquisas tenham crescido, a AES ainda é subexplorada devido às dificuldades em acessar conjuntos de dados e recursos computacionais. Muitos dos trabalhos nacionais foram desenvolvidos com foco no modelo de redações do ENEM, valendo-se de \textit{corpus} abertos de textos de portais de correção, como o \href{https://educacao.uol.com.br/bancoderedacoes/}{UOL Redação}.

\subsection{Definição e Importância}

A avaliação automática de redações é o processo no qual algoritmos computacionais são empregados para analisar e pontuar ensaios escritos em linguagem natural. Seu ciclo operacional engloba principalmente duas fases: a codificação dos textos, transformando-os em representações compreensíveis para os computadores, e a extração de informações, cujo propósito é identificar elementos relevantes no conteúdo textual para a atribuição da pontuação.

O cerne da AES reside na capacidade de desenvolver modelos capazes de atribuir notas automaticamente, em conformidade com critérios previamente estabelecidos. Tais critérios podem ser fundamentados em modelos estatísticos ou em técnicas de aprendizado de máquina supervisionado, em que os algoritmos são treinados com conjuntos de textos já avaliados por especialistas. É possível, dessa forma, desenvolver sistemas que aprendam os padrões e nuances que guiam a correção das redações.

A importância da área encontra-se na necessidade de enfrentar desafios logísticos e temporais associados à correção manual de redações, especialmente em contextos educacionais massivos, como ocorre no ENEM. A utilização de sistemas automáticos de avaliação não apenas agiliza o processo de correção, reduzindo o tempo necessário para a atribuição de notas, mas também alivia o esforço demandado por profissionais, como levantado em \cite{costa-et-al-2020}.

A objetividade, característica-chave de muitos critérios de avaliação de redações, também é um recurso importante dos sistemas automáticos. A AES contribui para uma correção mais uniforme e padronizada, minimizando possíveis variações subjetivas entre diferentes avaliadores e facilitando o processo de generalização exigido dos especialistas (\cite{myers-2003}). Além disso, a capacidade de processar grandes volumes de redações de forma eficiente a torna uma ferramenta valiosa em contextos educacionais de larga escala, onde a demanda por correção é significativa.

Em suma, a avaliação automática de redações é um campo de pesquisa e aplicação promissor, integrando conhecimentos de linguística, processamento de linguagem natural, aprendizado de máquina e estatística para endereçar desafios educacionais contemporâneos.

\subsection{Breve Histórico}

O surgimento da AES remonta a pesquisas pioneiras da década de 1960, destacando-se o \textit{Project Essay Grader} (PEG), desenvolvido para aprimorar a avaliação em larga escala de redações (\cite{page-1966}). O PEG utiliza medidas como comprimento médio de palavras e extensão dos textos para prever a qualidade das produções. Embora tenha sido elogiado, à época, por sua comparabilidade com avaliações humanas e eficiência computacional, a primeira versão do PEG recebeu críticas por negligenciar aspectos semânticos e por sua vulnerabilidade a práticas fraudulentas. Posteriormente, na década de 1990, esses problemas foram mitigados com a incorporação de dicionários e esquemas especiais de classificação ao sistema (\cite{felicia-et-al-2002}).

Um marco significativo no desenvolvimento da avaliação automática de redações no contexto nacional foi a pesquisa conduzida por \citet{amorim-et-al-2013}, com o desenvolvimento de um classificador bayesiano com cerca de 400 redações extraídas da base de dados do UOL Redações.

Subsequentemente, \citet{amorim-et-al-2017} propuseram um modelo baseado em regressão, treinado com 1840 textos. O trabalho foi realizado incorporando-se, para cada produção, duas classes de características: as que dizem respeito ao ENEM, chamadas de específicas de domínio, e as gerais, inspiradas no trabalho de \citet{attali-burstein-2006}.

Paralelamente, \citet{junior-et-al-2017} utilizaram técnicas de Máquinas de Vetores de Suporte (SVM) com cerca de 4000 redações extraídas do portal UOL, empregando o corretor gramatical CoGrOO (\cite{silva-2013}) para avaliar aspectos ortográficos do texto. Esse ensaio focou apenas na primeira competência do ENEM, que qualifica a adequação à modalidade formal da língua portuguesa.

Em outro estudo relevante, conduzido por \citet{fonseca-et-al-2018}, foram adotadas duas abordagens distintas. Em uma delas, os autores implementaram uma arquitetura de rede neural profunda com camadas bidirecionais de Long Short-Term Memory (BiLSTM). Na outra, criaram 681 características para alimentar um regressor na avaliação de redações, atingindo um resultado promissor e de melhor desempenho. Este trabalho utilizou uma base de cerca de 56000 redações para treinar o sistema.

No contexto do Brasil, apesar do avanço recente da área de AES, muitos dos estudos anteriores não disponibilizaram publicamente os conjuntos de textos utilizados, o que gera desafios para comparações robustas entre os trabalhos. Além disso, torna-se um desafio utilizar novas técnicas de avaliacão automática devido à escassez de dados representativos em língua portuguesa.

\section{Processamento de Linguagem Natural no Computador}

Ao lidarmos com textos no âmbito computacional, é crucial convertê-los em formatos adequados às máquinas. Algumas abordagens empregadas em NLP são os \textit{tokens}, os $n$-gramas, os \textit{one-hot encodings} e os \textit{word embeddings}.

\subsection{\textit{Tokens} e a Representação de Palavras}

\textit{Tokens} são unidades fundamentais de um texto, que podem englobar tanto palavras completas quanto partes delas. Formalmente, considerando um texto $T$ composto por $n$ elementos $t_1, t_2, \cdots, t_n$ que possam ser separados de forma discreta, os \textit{tokens} podem ser definidos como o conjunto $\{t_1, t_2, ..., t_n\}$ (\cite{manning-schuetze-1999}). De modo mais simples, \textit{tokens} são os elementos individuais de um texto, que podem ser divididos de forma a isolar unidades relevantes para um processamento.

O conceito de \textit{tokens} é crucial na representação de palavras, possibilitando que o computador processe e compreenda o conteúdo textual de maneira estruturada. Sua identificação e segmentação adequadas são vitais para tarefas de processamento de linguagem natural, como contagem de frequência, análise sintática e desenvolvimento de modelos.

Ao lidar com \textit{tokens}, é possível aplicar técnicas como a tokenização, que consiste na subdivisão de um texto em unidades individuais. Essa abordagem facilita a manipulação e análise, permitindo que algoritmos processem informações linguísticas de maneira mais eficaz.

\begin{figure}[H]
    \centering
    \caption{Exemplo de tokenização de uma frase em palavras, subpalavras e pontuações.}
    \label{fig:tokenizacao}
    \input{../figuras/tokenization}
\end{figure}

\subsection{$N$-gramas e Modelos de Predição de Palavras}

Os $N$-gramas, em NLP, são estruturas de agrupamento de palavras ou \textit{tokens} que desempenham um papel crucial na tarefa de previsão em textos. Tratam-se de uma junção de todos os elementos subjacentes possíveis, formando conjuntos de $n$ unidades. Essas representações são utilizadas, aliadas aos conceitos da teoria de Markov e das probabilidades, para treinar modelos simples de predição de palavras.

\begin{figure}[H]
    \centering
    \caption{Exemplos de $n$-gramas para $n=1, 2, 3$. Imagem extraída de \cite{fasttext-2018}.}
    \label{fig:bigrama}
    \includegraphics[width=0.6\textwidth]{../figuras/ngrams.png}
\end{figure}

O objetivo, nesse caso, é estimar a função de probabilidade $P(W_t | W_{t-(n-1)}, \cdots, W_{t-1})$, onde $W_t$ é a palavra atual e $W_{t-(n-1)}, \cdots, W_{t-1}$ representa o histórico de palavras. Sob a perspectiva estocástica, a classificação da história anterior ($W_{t-(n-1)}, \cdots, W_{t-1}$) é essencial para predizer as novas ocorrências, já que, com uma quantia suficientemente grande de textos, é possível estimar quais palavras tendem a aparecer em sequência.

No entanto, lidar com cada história textual separadamente é impraticável, já que é possível receber como entrada textos totalmente novos, sem construções de referência. A teoria de Markov, assim, surge como uma solução plausível, considerando que apenas o contexto local anterior, representado pelo conjunto das últimas palavras ($W_{t-(n-1)}, ..., W_{t-1}$), influencia a escolha da próxima ocorrência. Essa suposição leva em conta que a ordem de palavras em um texto é relevante e que geralmente existe maior correlação estatística entre unidades vizinhas, conforme pontuado em \cite{bengio-et-al-2003}.

Agrupando todas as histórias que compartilham as mesmas $n - 1$ palavras em uma mesma classe de equivalência, cria-se um modelo de Markov de ordem $n - 1$, conhecido como modelo de linguagem $N$-grama (\cite{manning-schuetze-1999}). Ao nomear tais modelos, a terminologia comumente utilizada refere-se a valores específicos de $n$, como bigrama para $n=2$ e trigrama para $n=3$. A abordagem de agrupar contextos semelhantes com base na teoria de Markov oferece uma maneira eficaz de prever palavras subsequentes em textos, com ampla aplicação no processamento de linguagem natural e análise de sentimentos.

\subsection{\textit{One-Hot Encodings}}

A técnica de \textit{one-hot encoding} é uma abordagem de representação palavras ou \textit{tokens} no âmbito do NLP. Nesse mecanismo, cada elemento do vocabulário é mapeado para um vetor binário distinto, onde todos os valores são zero, exceto aquele correspondente à posição da palavra de interesse, para a qual atribui-se o valor 1. Esse método cria representações vetoriais esparsas, cuja dimensionalidade é equivalente ao tamanho do vocabulário.

A principal característica do \textit{one-hot encoding} é a independência entre as representações de palavras. Cada uma delas é tratada como uma entidade única, sem consideração pela semelhança semântica ou relações contextuais com outras unidades do vocabulário. A representação binária resultante destaca a presença ou ausência de palavras específicas, mas não incorpora informações sobre o significado relativo delas ou suas interações semânticas.

Quando usado para codificar características ou classes, o \textit{one-hot encoding} pode ser considerado mais interpretável, dada sua natureza de independência explícita. A representação binária atribui um valor de 1 à categoria de interesse, com sua devida especificação, e 0 para todas as outras categorias, individualizando a presença ou ausência de uma característica e aumentando a interpretação de modelos, como mostrado por \citet{manai2023impact}.

Apesar de sua simplicidade e interpretabilidade, o \textit{one-hot encoding} apresenta desvantagens significativas em termos de eficiência computacional e capacidade de generalização. A representação esparsa resulta em um alto consumo de memória, especialmente em vocabulários extensos, e a falta de captura de relações semânticas limita sua utilidade em tarefas mais complexas de NLP.

\subsection{\textit{Word Embeddings}}

\textit{Word embeddings} referem-se a representações vetoriais de palavras que capturam relações semânticas e contextuais. Distintas dos $N$-gramas, que se restringem a contextos locais, essas representações incorporam informações abrangentes de todo o texto.

A origem dessas codificações remonta aos estágios iniciais dos modelos neurais probabilísticos de linguagem. Inspirados nas redes neurais de compressão de texto propostas por \citet{schmidhuber-heil-1996}, \citet{bengio-et-al-2003} conceberam um modelo de predição de palavras com uma estrutura análoga. Nesse trabalho, uma camada de projeção foi introduzida na rede neural, desempenhando o papel de mapear palavras para vetores de baixa dimensão.

Essa abordagem serviu como fundamento para o desenvolvimento de técnicas mais refinadas de \textit{word embedding}, que se disseminaram amplamente em aplicações de NLP. A representação matemática de palavras, expressa como vetores, proporciona não apenas manipulações computacionais eficientes, mas também a modelagem de aspectos semânticos pertinentes aos textos.

Lidar com representações vetoriais reais de palavras viabiliza a compreensão e exploração de relações semânticas complexas. Tais codificações permitem expressar analogias e operações sobre os elementos, como no exemplo da soma das palavras ``rei'' e ``mulher'', que resulta em vetores próximos da representação de ``rainha'' (\cite{mikolov-etal-2013-linguistic}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{../figuras/wordembeddings.png}
    \caption{Representação vetorial de palavras sob alguns eixos semânticos. Imagem extraída de \url{https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space}}
    \label{fig:word_embeddings}
\end{figure}

O emprego de \textit{word embeddings} é, atualmente, mais frequente, já que o método confere vantagens significativas em termos de manipulação e compreensão semântica. Representações semelhantes a essa estrutura são empregadas em modelos de linguagem mais avançados, como o BERT. Três métodos notáveis de \textit{word embedding} são o Word2Vec, GloVe e FastText.

\subsubsection{Word2Vec}

O Word2Vec é uma técnica de \textit{word embedding} que realiza uma tradução de palavras para vetores em espaços contínuos. Este método, fundamentado em redes neurais, destaca-se por sua habilidade em preservar relações semânticas através de representações distribuídas (\cite{mikolov-etal-2013-linguistic}). O cerne do processo consiste na aprendizagem de representações que não apenas mapeiam palavras individuais, mas também capturam a proximidade semântica entre elas.

Essa codificação opera de modo a otimizar a representação vetorial de palavras com base em seu contexto geral, possibilitando a criação de \textit{embeddings} que capturam a semelhança lexical e aspectos relacionados ao significado. O modelo se destaca principalmente na incorporação de analogias e de consequências lógicas, tais quais ``\textbf{falar} está para \textbf{falado} como \textbf{escrever} está para \textbf{escrito}''.

\subsubsection{GloVe}

O GloVe (\textit{Global Vectors for Word Representation}) apresenta uma perspectiva distinta no panorama dos \textit{word embeddings}. Ele se destaca na geração de representações vetoriais fundamentadas em estatísticas globais de coocorrência de palavras em um \textit{corpus}. Essa abordagem permite uma visão abrangente das relações semânticas que transcendem contextos imediatos.

A metodologia do GloVe se baseia na construção de uma matriz de coocorrência, refletindo a probabilidade de duas palavras aparecerem juntas em um dado contexto (\cite{pennington-etal-2014-glove}). Por meio de técnicas algébricas, o modelo é usado para derivar \textit{embeddings} que melhor representem as relações semânticas capturadas por essa matriz. Dessa forma, transcende-se a limitação de representar palavras isoladamente, proporcionando uma visão holística das conexões semânticas de um texto.

\subsubsection{FastText}

O FastText, uma extensão evolutiva do Word2Vec, é um tipo de codificação que atua em nível de subpalavras. Cada palavra, nesse método, é representada por $n$-gramas de suas composições. Essa inovação confere à técnica uma notável flexibilidade na manipulação de palavras desconhecidas, capturando informações semânticas em camadas mais inferiores de um texto, e permite uma maior agilidade no processo de treinamento.

Considerar subpalavras no processo de geração de \textit{embeddings} capacita o FastText a representar os vocábulos como combinações de suas partes constituintes. Para a palavra \textit{where}, por exemplo, esse modelo pode considerar os trigramas \texttt{<wh, whe, her, ere, re>}, conforme delineado por \citet{bojanowski-etal-2016-enriching}. Isso se revela particularmente valioso em línguas com estrutura morfológica complexa, como a língua portuguesa, onde as palavras são formadas por múltiplos elementos significativos. Alguns vocábulos não conhecidos, nesse caso, podem ser construídos pelos $n$-gramas, dando poder de generalização ao modelo.

Além disso, o FastText possui uma eficácia singular no processo de treinamento, permitindo a criação de \textit{embeddings} em ambientes computacionais mais simples. Conforme evidenciado por \citet{joulin-etal-2016-bag}, métodos baseados em convoluções, tais quais os utilizados para treinar representações de Word2Vec, demonstram ser muito mais lentos que o FastText, que alcança tempos de treinamento da ordem de minutos em CPUs padrões de 20 \textit{threads}. Essa notável agilidade contribui significativamente para a adoção do FastText em tarefas de NLP que exijam rápidas codificações.

\section{Modelos de Linguagem Neurais}

Os modelos de linguagem neurais são uma classe de entidades estatísticas que atribuem probabilidades a cadeias de palavras. Em geral, dado uma sequência de $t$ \textit{tokens} $\boldsymbol{W} = w_{1}, \cdots, w_{t}$ e um vocabulário de tamanho $V$, atuam de modo a aproximar a função de distribuição de probabilidades de um novo \textit{token} $w_{t+1}$, indicado por $P(w_{t+1} | W)$ (\cite{sun-iyyer-2021}). Esses modelos utilizam uma função de composição, denotada por $h$, que é aplicada sobre a sequência $W$, produzindo uma saída $\boldsymbol{s} = h(\boldsymbol{W})$.

Como o intuito é obter uma aproximação da distribuição de probabilidades sobre o vocabulário, uma restrição fundamental é a de que a resposta $\boldsymbol{r}$ atribuída pelos modelos de linguagem, com $\boldsymbol{r} = (r_{1}, \cdots, r_{V}) \in \mathbb{R}^{V}$, tenha valores cuja soma total seja igual a 1. Para tal, utiliza-se, comumente, a função \textit{softmax} (\cite{softmax-bridle}), cujo objetivo é normalizar um vetor $\boldsymbol{z} = (z_{1}, \cdots, z_{n}) \in \mathbb{R}^{n}$:

\begin{equation}
    \label{eq:softmax}
    g(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \text{ , para todo $i \in \{1, \cdots, n\}$}
\end{equation}

A saída $\boldsymbol{s}$ é, então, utilizada para estimar a probabilidade de ocorrência de um novo \textit{token} $w_{t+1}$, conforme a equação \ref{eq:softmax}, a partir do mapeamento realizado por $g$. Esse processo gera a resposta $r = \widehat{P}(w_{t+1} | W)$, um vetor de distribuição de probabilidades aproximada para o \textit{token} $w_{t+1}$ sobre o vocabulário.

\subsection{Redes Neurais Tradicionais}

As redes neurais tradicionais foram as pioneiras no desenvolvimento de modelos de linguagem. Os autores \citet{bengio-et-al-2003} foram os primeiros a lidar com essa estrutura, propondo uma arquitetura mais simples com duas camadas principais. A primeira delas realiza uma tarefa de projeção, mapeando palavras para vetores de baixa dimensão. Já a segunda atua como uma camada oculta, que recebe uma sequência de vetores e produz uma saída com base em uma função de ativação.

A rede recebe como entrada representações em \textit{one-hot encoding} de palavras, que são multiplicadas por uma matriz de projeção $C \in \mathbb{R}^{d \times V}$, onde $d$ é a dimensão dos vetores de projeção e $V$ é o tamanho do vocabulário. Essa matriz é treinada para realizar o mapeamento linear de palavras para vetores de representação distribuída, como os \textit{word embeddings}. A saída da camada de projeção é uma matriz $X \in \mathbb{R}^{d \times t}$, onde $t$ é o tamanho da sequência de entrada.

A matriz $X$ é, então, utilizada como entrada para a camada oculta, composta por uma quantia fixa de unidades de processamento, convencionalmente chamadas de neurônios. Cada neurônio recebe a sequência de vetores $x_{i} \in \mathbb{R}^{d}$ e produz uma saída $h_{i} \in \mathbb{R}^{d}$, calculada por:

\begin{equation}
    \label{eq:nnlm}
    h_{i}(x_{i}) = \tanh \left( b + W x_{i} \right)
\end{equation}

onde $W \in \mathbb{R}^{d \times d}$ é uma matriz de pesos e $b \in \mathbb{R}^{d}$ é um vetor de viés. A saída dessa etapa, representada pela matriz $H \in \mathbb{R}^{d \times t}$, passa por uma camada de tratamento com a função \textit{softmax} (\ref{eq:softmax}), gerando, finalmente, um vetor que representa a aproximação da função de probabilidades.

A Figura \ref{fig:nnlm} ilustra a estrutura dessa rede neural:

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma rede neural tradicional para modelagem de linguagem. Imagem extraída de \cite{bengio-et-al-2003}.}
    \label{fig:nnlm}
    \includegraphics[width=0.6\textwidth]{../figuras/nnlm.pdf}
\end{figure}

O treinamento dessa arquitetura de redes neurais tradicionais, apresentada por \citet{bengio-et-al-2003}, é realizado por meio da busca iterativa do valor $\theta$ que maximize a verossimilhança em relação às probabilidades logarítmiticas de um \textit{corpus} de tamanho $T$, representada pela equação \ref{eq:loglikelihood}. O algoritmo utilizado para essa busca é o de subida estocástica do gradiente (\cite{machine-learning-in-action}).

\begin{equation}
    \label{eq:loglikelihood}
    \mathcal{L}(\theta) = \frac{1}{T}\sum_{t=1}^{T} \log \widehat{P}(w_{t} | w_{t-1}, \cdots, w_{1}; \theta)
\end{equation}

À época, esses modelos representaram um marco importante no âmbito do NLP. Além da abordagem pioneira na representação de palavras, futuramente estabelecidos como os \textit{embeddings}, sua capacidade de aprender funções que estimam probabilidades oferece uma base sólida para a compreensão e geração de linguagem.

Todavia, a utilização dessa abordagem para lidar com textos suscitou muitas limitações na área. Uma delas é a incapacidade de capturar a dependência temporal entre as palavras em uma sequência, já que essas redes processam cada entrada de forma independente, sem considerar a ordem ou a relação cronológica entre vocábulos. Isso dificulta o uso do modelo para análise de textos cuja estrutura sequencial é essencial para o entendimento, como na avaliação automática de redações.

Outro desafio enfrentado pelas redes neurais tradicionais é a dificuldade em lidar com \textit{corpus} de diferentes comprimentos. Como essas estruturas têm uma arquitetura fixa com um número predefinido de camadas e unidades, torna-se custoso processar sequências de diferentes tamanhos de forma eficiente, já que o modelo possui uma complexidade computacional exponencial em relação ao tamanho da entrada.

\subsection{Redes Neurais Recorrentes (RNN)}

Para superar as limitações das redes neurais tradicionais em representar relações temporais e de ordem, surgiram as Redes Neurais Recorrentes (RNNs), baseadas na arquitetura proposta por \citet{rumelhart-1986-learning}. As RNNs têm a habilidade de aprender padrões de qualquer sequência de dados, como genomas, séries temporais, entre outros. Tal capacidade as torna particularmente adequadas para lidar, também, com tarefas de compreensão de textos, tendo em vista o caráter sequencial dessas estruturas (\cite{zhang2023dive}).

A arquitetura se diferencia principalmente pela introdução de um componente de memória, que é atualizado a cada novo elemento da sequência. A cada passo, a saída gerada pela rede realimenta a entrada, fazendo com que as RNNs levem em conta não só o novo dado, mas também as informações já processadas anteriormente. Assim, para um passo de tempo $t$ e uma função de ativação $f$, a saída da rede $h_t$ é calculada pela equação \ref{eq:rnn}:

\begin{equation}
    \label{eq:rnn}
    h_t = f(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})
\end{equation}

em que $x_t$ é a entrada no tempo $t$, $W_{ih}$ os pesos da camada oculta, $W_{hh}$ os pesos da ciclagem de informação (de camada oculta para camada oculta), $b_{ih}$ os vieses referentes à entrada atual e $b_{hh}$ os vieses relacionados com o histórico temporal. O termo $h_{t-1}$ representa o estado oculto anterior, incorporando informações das etapas de tempo passadas.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma RNN em formato simplificado e em formato sequencial aberto. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:rnns}
    \input{../figuras/rnns}
\end{figure}

Para o treinamento, as RNNs utilizam o algoritmo \textit{backpropagation through time} (BPTT), que é uma variação do algoritmo de retropropagação de redes tradicionais (\cite{schmidt2019recurrent}). O BPTT, de modo geral, atua criando uma versão sequencial aberta das RNNs ao longo do tempo, conforme mostrado na figura \ref{fig:rnns}. Essa representação é, então, tratada como uma estrutura tradicional, permitindo a aplicação do algoritmo de \textit{backpropagation} comum.

Algumas variações de RNNs, como as \textit{Bidirectional Recurrent Neural Networks} (BRNN), também surgiram para atender outras demandas específicas da área de NLP, como a previsão de palavras em meio de sequências. Esses modelos incorporam a capacidade de processar a informação tanto no sentido direto quanto no inverso, permitindo que a previsão de uma palavra considere não apenas o contexto precedente, mas também o subsequente. A estrutura esquemática das BRNNs pode ser vista na figura \ref{fig:brnn}.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma \textit{Bidirectional Recurrent Neural Network} (BRNN). Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:brnn}
    \input{../figuras/brnns}
\end{figure}

A possibilidade de manter estados internos ao longo das sequências permite que as RNNs capturem dependências de longo alcance em textos, um desafio enfrentado pelas redes neurais tradicionais. Além disso, a abordagem temporal habilita a utilização delas em \textit{corpus} de tamanhos variáveis, garantindo que se adaptem a diferentes contextos.

No entanto, a arquitetura recorrente também apresenta desafios, sendo o mais proeminente o problema de desvanecimento ou explosão do gradiente. Durante o treinamento, os vetores responsáveis por minimizar a função de perda podem se tornar extremamente pequenos ou grandes, dificultando a atualização eficiente dos parâmetros da rede. Por estarem associadas aos pesos da camada oculta, essas variações resultam em dificuldades para aprender dependências temporais de longo prazo, já que o histórico pode ser pouco relevante ou muito influente, a depender dos gradientes. Isso compromete principalmente a capacidade da RNN de reter informações pertinentes em sequências extensas, como já pontuado por \citet{schmidt2019recurrent}.

Ademais, as redes neurais recorrentes podem ser consideradas ineficientes e pouco escaláveis em relação a outras abordagens. Devido ao seu caráter sequencial, elas não conseguem processar entradas paralelamente, o que aumenta o tempo de treino de forma substancial.

Para superar o problema do desvanecimento do gradiente, foram propostas variantes de RNNs, como as redes \textit{Long Short-Term Memory} (LSTM) e as \textit{Gated Recurrent Units} (GRU).

\subsubsection{\textit{Long Short-Term Memory} (LSTM)}

As LSTMs são arquiteturas de RNNs que introduzem unidades de memória atualizadas e controladas por meio de portas (\cite{hochreiter1997long}). Isso permite que elas capturem relações de longo prazo em sequências sem uma propagação de erros que cresça descomedidamente, tornando-as especialmente eficazes em tarefas de NLP que usam a abordagem recorrente.

A estrutura de uma LSTM é formada por células de memória, que são envolvidas por três portas: de entrada ($\boldsymbol{I}_{t}$), esquecimento ($\boldsymbol{F}_{t}$) e saída ($\boldsymbol{O}_{t}$). A porta de entrada controla a atualização da célula de memória, a de esquecimento regula a retenção de informações em relação às computações anteriores e a de saída determina o valor final da unidade.

\begin{equation}
    \label{eq:lstm_input}
    \boldsymbol{I}_{t} = f \left( \boldsymbol{W}_{xi} \boldsymbol{X}_{t} + \boldsymbol{W}_{hi} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{i} \right)
\end{equation}
\begin{equation}
    \label{eq:lstm_forget}
    \boldsymbol{F}_{t} = f \left( \boldsymbol{W}_{xf} \boldsymbol{X}_{t} + \boldsymbol{W}_{hf} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{f} \right)
\end{equation}
\begin{equation}
    \label{eq:lstm_output}
    \boldsymbol{O}_{t} = f \left( \boldsymbol{W}_{xo} \boldsymbol{X}_{t} + \boldsymbol{W}_{ho} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{o} \right)
\end{equation}

As portas são calculadas pelas equações \ref{eq:lstm_input}, \ref{eq:lstm_forget} e \ref{eq:lstm_output}, respectivamente, em que $\boldsymbol{W}_{xi}$, $\boldsymbol{W}_{hi}$, $\boldsymbol{W}_{xf}$, $\boldsymbol{W}_{hf}$, $\boldsymbol{W}_{xo}$ e $\boldsymbol{W}_{ho}$ são matrizes de pesos e $\boldsymbol{b}_{i}$, $\boldsymbol{b}_{f}$ e $\boldsymbol{b}_{o}$ são vetores de viés. A função de ativação $f$ utilizada nas portas é a sigmoide, dada pela equação \ref{eq:sigmoid}, que retorna valores entre 0 e 1. Ela é utilizada para controlar a quantidade de informação que deve ser passada para a célula de memória.

\begin{equation}
    \label{eq:sigmoid}
    f(x) = \frac{1}{1 + e^{-x}}
\end{equation}

Conforme pontuado em \cite{schmidt2019recurrent}, as LSTMs têm, ainda, um componente de memória $\boldsymbol{\tilde{C}}_{t}$, cuja atualização é controlada pela combinação com as portas da célula. O principal objetivo desse elemento é regular a quantia de informação que deve ser passada para um novo estado de memória $\boldsymbol{C}_{t}$ com base no estado anterior $\boldsymbol{C}_{t-1}$. O cálculo de $\boldsymbol{\tilde{C}}_{t}$ é dado pela equação \ref{eq:lstm_ctilde}:

\begin{equation}
    \label{eq:lstm_ctilde}
    \boldsymbol{\tilde{C}}_{t} = \tanh \left( \boldsymbol{W}_{xc} \boldsymbol{X}_{t} + \boldsymbol{W}_{hc} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{c} \right)
\end{equation}

em que $\boldsymbol{W}_{xc}$ e $\boldsymbol{W}_{hc}$ são matrizes de pesos e $\boldsymbol{b}_{c}$ é um vetor de viés. A função de ativação de tangente hiperbólica é utilizada para normalizar os valores da célula de memória, retornando valores entre -1 e 1. Por fim, o estado interno da célula de memória $\boldsymbol{C}_{t}$ é calculado pela expressão \ref{eq:lstm_c} e a saída da unidade $\boldsymbol{H}_{t}$ é dada pela equação \ref{eq:lstm_h}, em que $\odot$ é a multiplicação elemento a elemento, chamada de produto de Hadamard.

\begin{equation}
    \label{eq:lstm_c}
    \boldsymbol{C}_{t} = \boldsymbol{F}_{t} \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_{t} \odot \boldsymbol{\tilde{C}}_{t}
\end{equation}

\begin{equation}
    \label{eq:lstm_h}
    \boldsymbol{H}_{t} = \boldsymbol{O}_{t} \odot \tanh(\boldsymbol{C}_{t})
\end{equation}

A Figura \ref{fig:lstm_architecture} ilustra a arquitetura geral das LSTMs.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma célula de memória das redes \textit{Long Short-Term Memory} (LSTM) em um dado tempo. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:lstm_architecture}
    \input{../figuras/lstm}
\end{figure}

A capacidade de aprender dependências de longo prazo em sequências destaca as LSTMs das redes neurais recorrentes convencionais. A estrutura de memória permite que elas capturem relações entre elementos distantes em uma sequência sem que o gradiente desvaneça ou cresça excessivamente. Isso é, em particular, importante para lidar com textos extensos, como os de uma redação.

Entretanto, as LSTMs não são isentas de desvantagens. Uma delas é a complexidade computacional, já que a estrutura de memória e as portas adicionam uma quantidade considerável de parâmetros ao modelo. Somado a isso, assim como no caso das RNNs convencionais, a arquitetura sequencial dessas redes não permite um processamento paralelo, agravando o problema de eficiência no treino.

\subsubsection{\textit{Gated Recurrent Unit} (GRU)}

Outra variação de RNNs é a \textit{Gated Recurrent Unit} (GRU), que visa simplificar a arquitetura das LSTMs, mantendo uma desempenho de avaliação semelhante. A GRU possui menos parâmetros e apenas duas portas: uma de atualização e uma de reinício (\cite{cho2014learning}). Isso a torna mais eficiente em termos computacionais e, em muitos casos, tão eficaz quanto as LSTMs.

A estrutura de uma GRU possui unidades de memória controladas por duas portas principais: a de atualização $\boldsymbol{Z}_{t}$ e a de reinício $\boldsymbol{R}_{t}$. Elas regulam o fluxo de informação de maneira eficiente à medida que atuam com menos parâmetros, sem perder a propriedade das LSTMs de assimilação de relações a longo prazo.

Em geral, a porta de atualização ajuda a capturar dependências de longo prazo, enquanto a de reinício auxilia na modelagem de dependências de curto prazo. A porta de atualização é calculada pela equação \ref{eq:gru_update}, enquanto a de reinício é dada pela equação \ref{eq:gru_reset}.

\begin{equation}
    \label{eq:gru_update}
    \boldsymbol{Z}_{t} = f \left( \boldsymbol{W}_{xr} \boldsymbol{X}_{t} + \boldsymbol{W}_{hr} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{r} \right)
\end{equation}
\begin{equation}
    \label{eq:gru_reset}
    \boldsymbol{R}_{t} = f \left( \boldsymbol{W}_{xz} \boldsymbol{X}_{t} + \boldsymbol{W}_{hz} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{z} \right)
\end{equation}

em que $\boldsymbol{W}_{xr}$, $\boldsymbol{W}_{hr}$, $\boldsymbol{W}_{xz}$ e $\boldsymbol{W}_{hz}$ são matrizes de pesos, $\boldsymbol{b}_{r}$ e $\boldsymbol{b}_{z}$ são vetores de viés e $f$ é a função de ativação sigmoide (\ref{eq:sigmoid}). Ambas as expressões são utilizadas para calcular um estado oculto intermediário $\tilde{\boldsymbol{H}}_{t}$, que é uma versão atualizada do estado anterior $\boldsymbol{H}_{t-1}$, conforme a equação \ref{eq:gru_hidden}.

\begin{equation}
    \label{eq:gru_hidden}
    \tilde{\boldsymbol{H}}_{t} = \tanh \left( \boldsymbol{W}_{xh} \boldsymbol{X}_{t} + \boldsymbol{W}_{hh} (\boldsymbol{R}_{t} \odot \boldsymbol{H}_{t-1}) + \boldsymbol{b}_{h} \right)
\end{equation}

Na expressão, $\boldsymbol{W}_{xh}$ e $\boldsymbol{W}_{hh}$ são matrizes de pesos e $\boldsymbol{b}_{h}$ é um vetor de viés. A atualização da unidade de memória $\boldsymbol{H}_{t}$ é, então, calculada pela equação \ref{eq:gru}, conforme \citet{zhang2023dive}

\begin{equation}
    \label{eq:gru}
    \boldsymbol{H}_{t} = (1 - \boldsymbol{Z}_{t}) \odot \tilde{\boldsymbol{H}}_{t} + \boldsymbol{Z}_{t} \odot \boldsymbol{H}_{t-1}
\end{equation}

A Figura \ref{fig:gru_architecture} apresenta a arquitetura da GRU, evidenciando as portas de atualização e reinício que controlam o fluxo de informação na unidade de memória.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma unidade de memória de uma \textit{Gated Recurrent Unit} (GRU) em um dado tempo. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:gru_architecture}
    \input{../figuras/gru}
\end{figure}

As GRUs oferecem uma alternativa competitiva às LSTMs em muitas aplicações, principalmente quando a eficiência no treinamento é um requisito chave, já que ambas têm um desempenho prático semelhante. Além disso, esse modelo possui uma implementação mais simples, dada a menor complexidade de parâmetros a serem operados.

No entanto, as GRUs podem ser mais limitadas em tarefas de NLP que envolvam dependências de longo prazo, já que as LSTMs têm uma estrutura de memória mais robusta capaz de assimilar adequadamente relações distantes em textos. Isso dificulta o uso desse modelo na área de avaliação automática de redações.

% \section{Modelos de Linguagem Baseados em Transformadores}

%\subsection{Arquitetura}

%\subsection{Mecanismo de Atenção}

%\section{BERT: Bidirectional Encoder Representations from Transformers}

%\subsection{Arquitetura}

%\subsection{Pré-treinamento}

%\subsection{Fine-tuning}

%\subsection{BERTimbau: BERT para o Português Brasileiro}

%\subsection{O BERT na Avaliação Automática de Redações}
