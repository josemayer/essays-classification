\chapter{Fundamentação Teórica}
\label{chap:fund}

\section{A Redação no ENEM}

A prova de redação no ENEM desempenha um papel fundamental na qualificação das habilidades dos estudantes, medindo a capacidade de expressarem ideias de maneira clara, coerente e persuasiva. Para avaliar as redações, são estabelecidos critérios objetivos que consideram a estrutura do texto e métricas específicas do exame, baseadas em uma matriz de referência.

\subsection{Estrutura do Texto}

A estrutura da redação no ENEM segue o formato dissertativo-argumentativo, exigindo que os participantes defendam um ponto de vista sobre um tema previamente estabelecido, conforme abordado em \cite{cartilha-redacao}. A composição textual deve respeitar a norma padrão da língua portuguesa e o gênero determinado, mantendo uma introdução, desenvolvimento e conclusão. A clareza na exposição de ideias, a adequação ao tema e a coesão textual são elementos essenciais para uma boa pontuação.

Para os autores \citet{platao-e-fiorin}, a dissertação deve explicitar uma visão concreta da realidade, recorrendo a referências para ilustrar afirmações ou para sustentar argumentos. No contexto do ENEM, esse é o papel exercido pela coesão e articulação da redação, já que o texto deve manter uma progressão lógica e um encadeamento de ideias com o fim de defender o ponto de vista do aluno. Além disso, os autores defendem que a dissertação sempre deve seguir um eixo temático à medida que aborda conceitos do mundo de modo abstrato, sem relações temporais ou espaciais. Nesse sentido, na prova, é imperativo que o texto apresente uma coerência temática, alinhando-se ao tópico proposto.

Ainda ao longo do texto, os estudantes devem articular informações externas para embasar seus discursos, o que é definido, em \cite{platao-e-fiorin}, como argumentos de autoridade e argumentos baseados em provas concretas. Assim, aliado à seleção de elementos coesivos adequados, o propósito da redação deve ser o de convencer o interlocutor em torno do tema abordado.

Por fim, é exigido que os alunos apresentem uma proposta de intervenção na conclusão do texto, que deve conter as ações necessárias para mitigar o problema levantado, os agentes responsáveis, os meios de solução, os possíveis desdobramentos da aplicação da proposta e, por fim, um detalhamento mais refinado da mediação (\cite[p.~20]{cartilha-redacao}).

\subsection{Métricas de Avaliação}

A avaliação das redações no ENEM adota uma abordagem multifacetada, incorporando diversas métricas para assegurar uma análise abrangente, incluindo desde a coesão textual até a capacidade de argumentação e a adequação ao padrão culto da língua. A seleção e organização eficazes de informações também são critérios fundamentais. A combinação desses parâmetros contribui significativamente para a construção de uma avaliação equitativa e holística das habilidades de escrita dos participantes.

Cada redação no ENEM é avaliada com base na matriz de referência do exame, composta por cinco competências, delineadas em suas respectivas habilidades, conforme detalhado em \cite{cartilha-redacao}. A primeira competência concentra-se na avaliação do emprego correto da língua portuguesa, abrangendo aspectos como ortografia, pontuação, concordância, regência verbal, etc. A segunda competência analisa a capacidade de interpretação do participante em relação ao tema proposto. Na terceira competência, são avaliados critérios como seleção e organização de informações, articulação de ideias e construção de argumentos. A quarta competência incide sobre o encadeamento dos argumentos e a progressão temática do texto. Por fim, a quinta competência avalia a habilidade do participante em elaborar uma proposta de intervenção para o problema abordado, considerando a adequação ao tema, a viabilidade prática de aplicação e o respeito aos direitos humanos. As cinco competências podem ser sintetizadas, de acordo com a Cartilha de Redação do ENEM (\cite{cartilha-redacao}), em:

\begin{enumerate}
    \item[\textbf{I.}] Demonstrar domínio da modalidade escrita formal da língua portuguesa.
    \item[\textbf{II.}] Compreender a proposta de redação e aplicar conceitos das várias áreas de conhecimento para desenvolver o tema, dentro dos limites estruturais do texto dissertativo-argumentativo em prosa.
    \item[\textbf{III.}] Selecionar, relacionar, organizar e interpretar informações, fatos, opiniões e argumentos em defesa de um ponto de vista.
    \item[\textbf{IV.}] Demonstrar conhecimento dos mecanismos linguísticos necessários para a construção da argumentação.
    \item[\textbf{V.}] Elaborar proposta de intervenção para o problema abordado, respeitando os direitos humanos.
\end{enumerate}

\subsection{Atribuição de Notas}

A atribuição de notas na redação do ENEM segue um processo rigoroso. Cada uma das cinco competências é avaliada em uma escala de 0 a 200 pontos, variando, nesse intervalo, em valores múltiplos de 40. As redações são corrigidas, inicialmente, por dois especialistas independentes e todo o processo é meticulosamente estruturado para prevenir possíveis divergências nas pontuações finais.

Duas avaliações são consideradas divergentes caso a nota atribuida a qualquer uma das competências difira em mais de 80 pontos ou se a diferença total entre as notas seja superior a 100 pontos. Nesses casos, uma terceira correção é realizada, e a nota final é a média aritmética das duas avaliações mais próximas. Caso todas as avaliações ainda sejam discrepantes entre si, a redação é submetida a uma banca independente, responsável por atribuir a nota final ao texto.

As notas levam em consideração as métricas da matriz de referência do exame, conforme estabelecido em \cite[p.~9-22]{cartilha-redacao}. A pontuação final varia de 0 a 1000 pontos, sendo calculada a partir da soma dos pontos atribuídos a cada competência. Este sistema de avaliação visa garantir uma análise abrangente e justa das redações dos participantes.

A tabela \ref{tab:competencia-1} abaixo, extraída da Cartilha de Redação do ENEM, ilustra a associação entre as pontuações atribuídas à competência I e os níveis de desempenho esperados dos alunos. As demais competências possuem uma associação similar de acordo com seus respectivos campos de avaliação.

\begin{table}[H]
    \centering
    \caption{Níveis de desempenho esperados para a competência I e notas associadas. Tabela extraída de \cite[p.~10]{cartilha-redacao}}
    \label{tab:competencia-1}
    \begin{tabularx}{\textwidth}{|c|X|}
        \hline
        \textbf{Pontuação} & \textbf{Níveis de desempenho} \\
        \hline
        200 pontos & Demonstra excelente domínio da modalidade escrita formal da língua portuguesa e de escolha de registro. Desvios gramaticais ou de convenções da escrita serão aceitos somente como excepcionalidade e quando não caracterizarem reincidência. \\
        \hline
        160 pontos & Demonstra bom domínio da modalidade escrita formal da língua portuguesa e de escolha de registro, com poucos desvios gramaticais e de convenções da escrita. \\
        \hline
        120 pontos & Demonstra domínio mediano da modalidade escrita formal da língua portuguesa e de escolha de registro, com alguns desvios gramaticais e de convenções da escrita. \\
        \hline
        80 pontos & Demonstra domínio insuficiente da modalidade escrita formal da língua portuguesa, com muitos desvios gramaticais, de escolha de registro e de convenções da escrita. \\
        \hline
        40 pontos & Demonstra domínio precário da modalidade escrita formal da língua portuguesa, de forma sistemática, com diversificados e frequentes desvios gramaticais, de escolha de registro e de convenções da escrita. \\
        \hline
        0 pontos & Demonstra desconhecimento da modalidade escrita formal da língua portuguesa. \\
        \hline
    \end{tabularx}
\end{table}

\section{Avaliação Automática de Redações}

A avaliação automática de redações (AES) tem se destacado significativamente nos últimos anos devido à disparidade entre a disponibilidade de profissionais capacitados para correção de textos e o número expressivo de estudantes. O avanço das técnicas de modelagem de linguagem e processamento de linguagem natural (PLN) impulsionou inúmeros trabalhos na tentativa de simplificar o processo de correção.

Este campo de pesquisa é intrinsecamente multidisciplinar, demandando conhecimentos em linguística, PLN, aprendizado de máquina e estatística. A meta central consiste em desenvolver modelos capazes de atribuir notas automaticamente a redações, seguindo critérios similares aos adotados por especialistas.

No cenário brasileiro, embora as pesquisas tenham crescido, a AES ainda é subexplorada devido às dificuldades em acessar conjuntos de dados e recursos computacionais. Muitos dos trabalhos nacionais foram desenvolvidos com foco no modelo de redações do ENEM, valendo-se de \textit{corpus} abertos de textos de portais de correção, como o \href{https://educacao.uol.com.br/bancoderedacoes/}{UOL Redação}.

\subsection{Definição e Importância}

A avaliação automática de redações é o processo no qual algoritmos computacionais são empregados para analisar e pontuar ensaios escritos em linguagem natural. Seu ciclo operacional engloba principalmente duas fases: a codificação dos textos, transformando-os em representações compreensíveis para os computadores, e a extração de informações, cujo propósito é identificar elementos relevantes no conteúdo textual para a atribuição da pontuação.

O cerne da AES reside na capacidade de desenvolver modelos capazes de atribuir notas automaticamente, em conformidade com critérios previamente estabelecidos. Tais critérios podem ser fundamentados em modelos estatísticos ou em técnicas de aprendizado de máquina supervisionado, em que os algoritmos são treinados com conjuntos de textos já avaliados por especialistas. É possível, dessa forma, desenvolver sistemas que aprendam os padrões e nuances que guiam a correção das redações.

A importância da área encontra-se na necessidade de enfrentar desafios logísticos e temporais associados à correção manual de redações, especialmente em contextos educacionais massivos, como ocorre no ENEM. A utilização de sistemas automáticos de avaliação não apenas agiliza o processo de correção, reduzindo o tempo necessário para a atribuição de notas, mas também alivia o esforço demandado por profissionais, como levantado em \cite{costa-et-al-2020}.

A objetividade, característica-chave de muitos critérios de avaliação de redações, também é um recurso importante dos sistemas automáticos. A AES contribui para uma correção mais uniforme e padronizada, minimizando possíveis variações subjetivas entre diferentes avaliadores e facilitando o processo de generalização exigido dos especialistas (\cite{myers-2003}). Além disso, a capacidade de processar grandes volumes de redações de forma eficiente a torna uma ferramenta valiosa em contextos educacionais de larga escala, onde a demanda por correção é significativa.

Em suma, a avaliação automática de redações é um campo de pesquisa e aplicação promissor, integrando conhecimentos de linguística, processamento de linguagem natural, aprendizado de máquina e estatística para endereçar desafios educacionais contemporâneos.

\subsection{Breve Histórico}

O surgimento da AES remonta a pesquisas pioneiras da década de 1960, destacando-se o \textit{Project Essay Grader} (PEG), desenvolvido para aprimorar a avaliação em larga escala de redações (\cite{page-1966}). O PEG utiliza medidas como comprimento médio de palavras e extensão dos textos para prever a qualidade das produções. Embora tenha sido elogiado, à época, por sua comparabilidade com avaliações humanas e eficiência computacional, a primeira versão do PEG recebeu críticas por negligenciar aspectos semânticos e por sua vulnerabilidade a práticas fraudulentas. Posteriormente, na década de 1990, esses problemas foram mitigados com a incorporação de dicionários e esquemas especiais de classificação ao sistema (\cite{felicia-et-al-2002}).

Um marco significativo no desenvolvimento da avaliação automática de redações no contexto nacional foi a pesquisa conduzida por \citet{amorim-et-al-2013}, com o desenvolvimento de um classificador bayesiano com cerca de 400 redações extraídas da base de dados do UOL Redações.

Subsequentemente, \citet{amorim-et-al-2017} propuseram um modelo baseado em regressão, treinado com 1840 textos. O trabalho foi realizado incorporando-se, para cada produção, duas classes de características: as que dizem respeito ao ENEM, chamadas de específicas de domínio, e as gerais, inspiradas no trabalho de \citet{attali-burstein-2006}.

Paralelamente, \citet{junior-et-al-2017} utilizaram técnicas de Máquinas de Vetores de Suporte (SVM) com cerca de 4000 redações extraídas do portal UOL, empregando o corretor gramatical CoGrOO (\cite{silva-2013}) para avaliar aspectos ortográficos do texto. Esse ensaio focou apenas na primeira competência do ENEM, que qualifica a adequação à modalidade formal da língua portuguesa.

Em outro estudo relevante, conduzido por \citet{fonseca-et-al-2018}, foram adotadas duas abordagens distintas. Em uma delas, os autores implementaram uma arquitetura de rede neural profunda com camadas bidirecionais de Long Short-Term Memory (BiLSTM). Na outra, criaram 681 características para alimentar um regressor na avaliação de redações, atingindo um resultado promissor e de melhor desempenho. Este trabalho utilizou uma base de cerca de 56000 redações para treinar o sistema.

No contexto do Brasil, apesar do avanço recente da área de AES, muitos dos estudos anteriores não disponibilizaram publicamente os conjuntos de textos utilizados, o que gera desafios para comparações robustas entre os trabalhos. Além disso, torna-se um desafio utilizar novas técnicas de avaliacão automática devido à escassez de dados representativos em língua portuguesa.

\section{Processamento de Linguagem Natural no Computador}

Ao lidarmos com textos no âmbito computacional, é crucial convertê-los em formatos adequados às máquinas. Algumas abordagens empregadas em NLP são os \textit{tokens}, os $n$-gramas, os \textit{one-hot encodings} e os \textit{word embeddings}.

\subsection{\textit{Tokens} e a Representação de Palavras}

\textit{Tokens} são unidades fundamentais de um texto, que podem englobar tanto palavras completas quanto partes delas. Formalmente, considerando um texto $T$ composto por $n$ elementos $t_1, t_2, \cdots, t_n$ que possam ser separados de forma discreta, os \textit{tokens} podem ser definidos como o conjunto $\{t_1, t_2, ..., t_n\}$ (\cite{manning-schuetze-1999}). De modo mais simples, \textit{tokens} são os elementos individuais de um texto, que podem ser divididos de forma a isolar unidades relevantes para um processamento.

O conceito de \textit{tokens} é crucial na representação de palavras, possibilitando que o computador processe e compreenda o conteúdo textual de maneira estruturada. Sua identificação e segmentação adequadas são vitais para tarefas de processamento de linguagem natural, como contagem de frequência, análise sintática e desenvolvimento de modelos.

Ao lidar com \textit{tokens}, é possível aplicar técnicas como a tokenização, que consiste na subdivisão de um texto em unidades individuais. Essa abordagem facilita a manipulação e análise, permitindo que algoritmos processem informações linguísticas de maneira mais eficaz.

\begin{figure}[H]
    \centering
    \caption{Exemplo de tokenização de uma frase em palavras, subpalavras e pontuações.}
    \label{fig:tokenizacao}
    \input{../figuras/tokenization}
\end{figure}

\subsection{$N$-gramas e Modelos de Predição de Palavras}

Os $N$-gramas, em NLP, são estruturas de agrupamento de palavras ou \textit{tokens} que desempenham um papel crucial na tarefa de previsão em textos. Tratam-se de uma junção de todos os elementos subjacentes possíveis, formando conjuntos de $n$ unidades. Essas representações são utilizadas, aliadas aos conceitos da teoria de Markov e das probabilidades, para treinar modelos simples de predição de palavras.

\begin{figure}[H]
    \centering
    \caption{Exemplos de $n$-gramas para $n=1, 2, 3$. Imagem extraída de \cite{fasttext-2018}.}
    \label{fig:bigrama}
    \includegraphics[width=0.6\textwidth]{../figuras/ngrams.png}
\end{figure}

O objetivo, nesse caso, é estimar a função de probabilidade $P(W_t | W_{t-(n-1)}, \cdots, W_{t-1})$, onde $W_t$ é a palavra atual e $W_{t-(n-1)}, \cdots, W_{t-1}$ representa o histórico de palavras. Sob a perspectiva estocástica, a classificação da história anterior ($W_{t-(n-1)}, \cdots, W_{t-1}$) é essencial para predizer as novas ocorrências, já que, com uma quantia suficientemente grande de textos, é possível estimar quais palavras tendem a aparecer em sequência.

No entanto, lidar com cada história textual separadamente é impraticável, já que é possível receber como entrada textos totalmente novos, sem construções de referência. A teoria de Markov, assim, surge como uma solução plausível, considerando que apenas o contexto local anterior, representado pelo conjunto das últimas palavras ($W_{t-(n-1)}, ..., W_{t-1}$), influencia a escolha da próxima ocorrência. Essa suposição leva em conta que a ordem de palavras em um texto é relevante e que geralmente existe maior correlação estatística entre unidades vizinhas, conforme pontuado em \cite{bengio-et-al-2003}.

Agrupando todas as histórias que compartilham as mesmas $n - 1$ palavras em uma mesma classe de equivalência, cria-se um modelo de Markov de ordem $n - 1$, conhecido como modelo de linguagem $N$-grama (\cite{manning-schuetze-1999}). Ao nomear tais modelos, a terminologia comumente utilizada refere-se a valores específicos de $n$, como bigrama para $n=2$ e trigrama para $n=3$. A abordagem de agrupar contextos semelhantes com base na teoria de Markov oferece uma maneira eficaz de prever palavras subsequentes em textos, com ampla aplicação no processamento de linguagem natural e análise de sentimentos.

\subsection{\textit{One-Hot Encodings}}

A técnica de \textit{one-hot encoding} é uma abordagem de representação palavras ou \textit{tokens} no âmbito do NLP. Nesse mecanismo, cada elemento do vocabulário é mapeado para um vetor binário distinto, onde todos os valores são zero, exceto aquele correspondente à posição da palavra de interesse, para a qual atribui-se o valor 1. Esse método cria representações vetoriais esparsas, cuja dimensionalidade é equivalente ao tamanho do vocabulário.

A principal característica do \textit{one-hot encoding} é a independência entre as representações de palavras. Cada uma delas é tratada como uma entidade única, sem consideração pela semelhança semântica ou relações contextuais com outras unidades do vocabulário. A representação binária resultante destaca a presença ou ausência de palavras específicas, mas não incorpora informações sobre o significado relativo delas ou suas interações semânticas.

Quando usado para codificar características ou classes, o \textit{one-hot encoding} pode ser considerado mais interpretável, dada sua natureza de independência explícita. A representação binária atribui um valor de 1 à categoria de interesse, com sua devida especificação, e 0 para todas as outras categorias, individualizando a presença ou ausência de uma característica e aumentando a interpretação de modelos, como mostrado por \citet{manai2023impact}.

Apesar de sua simplicidade e interpretabilidade, o \textit{one-hot encoding} apresenta desvantagens significativas em termos de eficiência computacional e capacidade de generalização. A representação esparsa resulta em um alto consumo de memória, especialmente em vocabulários extensos, e a falta de captura de relações semânticas limita sua utilidade em tarefas mais complexas de NLP.

\subsection{\textit{Word Embeddings}}

\textit{Word embeddings} referem-se a representações vetoriais de palavras que capturam relações semânticas e contextuais. Distintas dos $N$-gramas, que se restringem a contextos locais, essas representações incorporam informações abrangentes de todo o texto.

A origem dessas codificações remonta aos estágios iniciais dos modelos neurais probabilísticos de linguagem. Inspirados nas redes neurais de compressão de texto propostas por \citet{schmidhuber-heil-1996}, \citet{bengio-et-al-2003} conceberam um modelo de predição de palavras com uma estrutura análoga. Nesse trabalho, uma camada de projeção foi introduzida na rede neural, desempenhando o papel de mapear palavras para vetores de baixa dimensão.

Essa abordagem serviu como fundamento para o desenvolvimento de técnicas mais refinadas de \textit{word embedding}, que se disseminaram amplamente em aplicações de NLP. A representação matemática de palavras, expressa como vetores, proporciona não apenas manipulações computacionais eficientes, mas também a modelagem de aspectos semânticos pertinentes aos textos.

Lidar com representações vetoriais reais de palavras viabiliza a compreensão e exploração de relações semânticas complexas. Tais codificações permitem expressar analogias e operações sobre os elementos, como no exemplo da soma das palavras ``rei'' e ``mulher'', que resulta em vetores próximos da representação de ``rainha'' (\cite{mikolov-etal-2013-linguistic}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{../figuras/wordembeddings.png}
    \caption{Representação vetorial de palavras sob alguns eixos semânticos. Imagem extraída de \url{https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space}}
    \label{fig:word_embeddings}
\end{figure}

O emprego de \textit{word embeddings} é, atualmente, mais frequente, já que o método confere vantagens significativas em termos de manipulação e compreensão semântica. Representações semelhantes a essa estrutura são empregadas em modelos de linguagem mais avançados, como o BERT. Três métodos notáveis de \textit{word embedding} são o Word2Vec, GloVe e FastText.

\subsubsection{Word2Vec}

O Word2Vec é uma técnica de \textit{word embedding} que realiza uma tradução de palavras para vetores em espaços contínuos. Este método, fundamentado em redes neurais, destaca-se por sua habilidade em preservar relações semânticas através de representações distribuídas (\cite{mikolov-etal-2013-linguistic}). O cerne do processo consiste na aprendizagem de representações que não apenas mapeiam palavras individuais, mas também capturam a proximidade semântica entre elas.

Essa codificação opera de modo a otimizar a representação vetorial de palavras com base em seu contexto geral, possibilitando a criação de \textit{embeddings} que capturam a semelhança lexical e aspectos relacionados ao significado. O modelo se destaca principalmente na incorporação de analogias e de consequências lógicas, tais quais ``\textbf{falar} está para \textbf{falado} como \textbf{escrever} está para \textbf{escrito}''.

\subsubsection{GloVe}

O GloVe (\textit{Global Vectors for Word Representation}) apresenta uma perspectiva distinta no panorama dos \textit{word embeddings}. Ele se destaca na geração de representações vetoriais fundamentadas em estatísticas globais de coocorrência de palavras em um \textit{corpus}. Essa abordagem permite uma visão abrangente das relações semânticas que transcendem contextos imediatos.

A metodologia do GloVe se baseia na construção de uma matriz de coocorrência, refletindo a probabilidade de duas palavras aparecerem juntas em um dado contexto (\cite{pennington-etal-2014-glove}). Por meio de técnicas algébricas, o modelo é usado para derivar \textit{embeddings} que melhor representem as relações semânticas capturadas por essa matriz. Dessa forma, transcende-se a limitação de representar palavras isoladamente, proporcionando uma visão holística das conexões semânticas de um texto.

\subsubsection{FastText}

O FastText, uma extensão evolutiva do Word2Vec, é um tipo de codificação que atua em nível de subpalavras. Cada palavra, nesse método, é representada por $n$-gramas de suas composições. Essa inovação confere à técnica uma notável flexibilidade na manipulação de palavras desconhecidas, capturando informações semânticas em camadas mais inferiores de um texto, e permite uma maior agilidade no processo de treinamento.

Considerar subpalavras no processo de geração de \textit{embeddings} capacita o FastText a representar os vocábulos como combinações de suas partes constituintes. Para a palavra \textit{where}, por exemplo, esse modelo pode considerar os trigramas \texttt{<wh, whe, her, ere, re>}, conforme delineado por \citet{bojanowski-etal-2016-enriching}. Isso se revela particularmente valioso em línguas com estrutura morfológica complexa, como a língua portuguesa, onde as palavras são formadas por múltiplos elementos significativos. Alguns vocábulos não conhecidos, nesse caso, podem ser construídos pelos $n$-gramas, dando poder de generalização ao modelo.

Além disso, o FastText possui uma eficácia singular no processo de treinamento, permitindo a criação de \textit{embeddings} em ambientes computacionais mais simples. Conforme evidenciado por \citet{joulin-etal-2016-bag}, métodos baseados em convoluções, tais quais os utilizados para treinar representações de Word2Vec, demonstram ser muito mais lentos que o FastText, que alcança tempos de treinamento da ordem de minutos em CPUs padrões de 20 \textit{threads}. Essa notável agilidade contribui significativamente para a adoção do FastText em tarefas de NLP que exijam rápidas codificações.

\section{Modelos de Linguagem Neurais}

Os modelos de linguagem neurais são uma classe de entidades estatísticas que atribuem probabilidades a cadeias de palavras. Em geral, dado uma sequência de $t$ \textit{tokens} $\boldsymbol{W} = w_{1}, \cdots, w_{t}$ e um vocabulário de tamanho $V$, atuam de modo a aproximar a função de distribuição de probabilidades de um novo \textit{token} $w_{t+1}$, indicado por $P(w_{t+1} | W)$ (\cite{sun-iyyer-2021}). Esses modelos utilizam uma função de composição, denotada por $h$, que é aplicada sobre a sequência $W$, produzindo uma saída $\boldsymbol{s} = h(\boldsymbol{W})$.

Como o intuito é obter uma aproximação da distribuição de probabilidades sobre o vocabulário, uma restrição fundamental é a de que a resposta $\boldsymbol{r}$ atribuída pelos modelos de linguagem, com $\boldsymbol{r} = (r_{1}, \cdots, r_{V}) \in \mathbb{R}^{V}$, tenha valores cuja soma total seja igual a 1. Para tal, utiliza-se, comumente, a função \textit{softmax} (\cite{softmax-bridle}), cujo objetivo é normalizar um vetor $\boldsymbol{z} = (z_{1}, \cdots, z_{n}) \in \mathbb{R}^{n}$:

\begin{equation}
    \label{eq:softmax}
    g(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \text{ , para todo $i \in \{1, \cdots, n\}$}
\end{equation}

A saída $\boldsymbol{s}$ é, então, utilizada para estimar a probabilidade de ocorrência de um novo \textit{token} $w_{t+1}$, conforme a equação \ref{eq:softmax}, a partir do mapeamento realizado por $g$. Esse processo gera a resposta $r = \widehat{P}(w_{t+1} | W)$, um vetor de distribuição de probabilidades aproximada para o \textit{token} $w_{t+1}$ sobre o vocabulário.

\subsection{Redes Neurais Tradicionais}

As redes neurais tradicionais foram as pioneiras no desenvolvimento de modelos de linguagem. Os autores \citet{bengio-et-al-2003} foram os primeiros a lidar com essa estrutura, propondo uma arquitetura mais simples com duas camadas principais. A primeira delas realiza uma tarefa de projeção, mapeando palavras para vetores de baixa dimensão. Já a segunda atua como uma camada oculta, que recebe uma sequência de vetores e produz uma saída com base em uma função de ativação.

A rede recebe como entrada representações em \textit{one-hot encoding} de palavras, que são multiplicadas por uma matriz de projeção $C \in \mathbb{R}^{d \times V}$, onde $d$ é a dimensão dos vetores de projeção e $V$ é o tamanho do vocabulário. Essa matriz é treinada para realizar o mapeamento linear de palavras para vetores de representação distribuída, como os \textit{word embeddings}. A saída da camada de projeção é uma matriz $X \in \mathbb{R}^{d \times t}$, onde $t$ é o tamanho da sequência de entrada.

A matriz $X$ é, então, utilizada como entrada para a camada oculta, composta por uma quantia fixa de unidades de processamento, convencionalmente chamadas de neurônios. Cada neurônio recebe a sequência de vetores $x_{i} \in \mathbb{R}^{d}$ e produz uma saída $h_{i} \in \mathbb{R}^{d}$, calculada por:

\begin{equation}
    \label{eq:nnlm}
    h_{i}(x_{i}) = \tanh \left( b + W x_{i} \right)
\end{equation}

onde $W \in \mathbb{R}^{d \times d}$ é uma matriz de pesos e $b \in \mathbb{R}^{d}$ é um vetor de viés. A saída dessa etapa, representada pela matriz $H \in \mathbb{R}^{d \times t}$, passa por uma camada de tratamento com a função \textit{softmax} (\ref{eq:softmax}), gerando, finalmente, um vetor que representa a aproximação da função de probabilidades.

A Figura \ref{fig:nnlm} ilustra a estrutura dessa rede neural:

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma rede neural tradicional para modelagem de linguagem. Imagem extraída de \cite{bengio-et-al-2003}.}
    \label{fig:nnlm}
    \includegraphics[width=0.6\textwidth]{../figuras/nnlm.pdf}
\end{figure}

O treinamento dessa arquitetura de redes neurais tradicionais, apresentada por \citet{bengio-et-al-2003}, é realizado por meio da busca iterativa do valor $\theta$ que maximize a verossimilhança em relação às probabilidades logarítmiticas de um \textit{corpus} de tamanho $T$, representada pela equação \ref{eq:loglikelihood}. O algoritmo utilizado para essa busca é o de subida estocástica do gradiente (\cite{machine-learning-in-action}).

\begin{equation}
    \label{eq:loglikelihood}
    \mathcal{L}(\theta) = \frac{1}{T}\sum_{t=1}^{T} \log \widehat{P}(w_{t} | w_{t-1}, \cdots, w_{1}; \theta)
\end{equation}

À época, esses modelos representaram um marco importante no âmbito do NLP. Além da abordagem pioneira na representação de palavras, futuramente estabelecidos como os \textit{embeddings}, sua capacidade de aprender funções que estimam probabilidades oferece uma base sólida para a compreensão e geração de linguagem.

Todavia, a utilização dessa abordagem para lidar com textos suscitou muitas limitações na área. Uma delas é a incapacidade de capturar a dependência temporal entre as palavras em uma sequência, já que essas redes processam cada entrada de forma independente, sem considerar a ordem ou a relação cronológica entre vocábulos. Isso dificulta o uso do modelo para análise de textos cuja estrutura sequencial é essencial para o entendimento, como na avaliação automática de redações.

Outro desafio enfrentado pelas redes neurais tradicionais é a dificuldade em lidar com \textit{corpus} de diferentes comprimentos. Como essas estruturas têm uma arquitetura fixa com um número predefinido de camadas e unidades, torna-se custoso processar sequências de diferentes tamanhos de forma eficiente, já que o modelo possui uma complexidade computacional exponencial em relação ao tamanho da entrada.

\subsection{Redes Neurais Recorrentes (RNN)}

Para superar as limitações das redes neurais tradicionais em representar relações temporais e de ordem, surgiram as Redes Neurais Recorrentes (RNNs), baseadas na arquitetura proposta por \citet{rumelhart-1986-learning}. As RNNs têm a habilidade de aprender padrões de qualquer sequência de dados, como genomas, séries temporais, entre outros. Tal capacidade as torna particularmente adequadas para lidar, também, com tarefas de compreensão de textos, tendo em vista o caráter sequencial dessas estruturas (\cite{zhang2023dive}).

A arquitetura se diferencia principalmente pela introdução de um componente de memória, que é atualizado a cada novo elemento da sequência. A cada passo, a saída gerada pela rede realimenta a entrada, fazendo com que as RNNs levem em conta não só o novo dado, mas também as informações já processadas anteriormente. Assim, para um passo de tempo $t$ e uma função de ativação $f$, a saída da rede $h_t$ é calculada pela equação \ref{eq:rnn}:

\begin{equation}
    \label{eq:rnn}
    h_t = f(\boldsymbol{W}_{ih} x_t + \boldsymbol{b}_{ih} + \boldsymbol{W}_{hh} h_{t-1} + \boldsymbol{b}_{hh})
\end{equation}

em que $x_t$ é a entrada no tempo $t$, $\boldsymbol{W}_{ih}$ os pesos da camada oculta, $\boldsymbol{W}_{hh}$ os pesos da ciclagem de informação (de camada oculta para camada oculta), $\boldsymbol{b}_{ih}$ os vieses referentes à entrada atual e $\boldsymbol{b}_{hh}$ os vieses relacionados com o histórico temporal. O termo $h_{t-1}$ representa o estado oculto anterior, incorporando informações das etapas de tempo passadas.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma RNN em formato simplificado e em formato sequencial aberto. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:rnns}
    \input{../figuras/rnns}
\end{figure}

Para o treinamento, as RNNs utilizam o algoritmo \textit{backpropagation through time} (BPTT), que é uma variação do algoritmo de retropropagação de redes tradicionais (\cite{schmidt2019recurrent}). O BPTT, de modo geral, atua criando uma versão sequencial aberta das RNNs ao longo do tempo, conforme mostrado na figura \ref{fig:rnns}. Essa representação é, então, tratada como uma estrutura tradicional, permitindo a aplicação do algoritmo de \textit{backpropagation} comum.

Algumas variações de RNNs, como as \textit{Bidirectional Recurrent Neural Networks} (BRNN), também surgiram para atender outras demandas específicas da área de NLP, como a previsão de palavras em meio de sequências. Esses modelos incorporam a capacidade de processar a informação tanto no sentido direto quanto no inverso, permitindo que a previsão de uma palavra considere não apenas o contexto precedente, mas também o subsequente. A estrutura esquemática das BRNNs pode ser vista na figura \ref{fig:brnn}.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma \textit{Bidirectional Recurrent Neural Network} (BRNN). Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:brnn}
    \input{../figuras/brnns}
\end{figure}

A possibilidade de manter estados internos ao longo das sequências permite que as RNNs capturem dependências de longo alcance em textos, um desafio enfrentado pelas redes neurais tradicionais. Além disso, a abordagem temporal habilita a utilização delas em \textit{corpus} de tamanhos variáveis, garantindo que se adaptem a diferentes contextos.

No entanto, a arquitetura recorrente também apresenta desafios, sendo o mais proeminente o problema de desvanecimento ou explosão do gradiente. Durante o treinamento, os vetores responsáveis por minimizar a função de perda podem se tornar extremamente pequenos ou grandes, dificultando a atualização eficiente dos parâmetros da rede. Por estarem associadas aos pesos da camada oculta, essas variações resultam em dificuldades para aprender dependências temporais de longo prazo, já que o histórico pode ser pouco relevante ou muito influente, a depender dos gradientes. Isso compromete principalmente a capacidade da RNN de reter informações pertinentes em sequências extensas, como já pontuado por \citet{schmidt2019recurrent}.

Ademais, as redes neurais recorrentes podem ser consideradas ineficientes e pouco escaláveis em relação a outras abordagens. Devido ao seu caráter sequencial, elas não conseguem processar entradas paralelamente, o que aumenta o tempo de treino de forma substancial.

Para superar o problema do desvanecimento do gradiente, foram propostas variantes de RNNs, como as redes \textit{Long Short-Term Memory} (LSTM) e as \textit{Gated Recurrent Units} (GRU).

\subsubsection{\textit{Long Short-Term Memory} (LSTM)}

As LSTMs são arquiteturas de RNNs que introduzem unidades de memória atualizadas e controladas por meio de portas (\cite{hochreiter1997long}). Isso permite que elas capturem relações de longo prazo em sequências sem uma propagação de erros que cresça descomedidamente, tornando-as especialmente eficazes em tarefas de NLP que usam a abordagem recorrente.

A estrutura de uma LSTM é formada por células de memória, que são envolvidas por três portas: de entrada ($\boldsymbol{I}_{t}$), esquecimento ($\boldsymbol{F}_{t}$) e saída ($\boldsymbol{O}_{t}$). A porta de entrada controla a atualização da célula de memória, a de esquecimento regula a retenção de informações em relação às computações anteriores e a de saída determina o valor final da unidade.

\begin{equation}
    \label{eq:lstm_input}
    \boldsymbol{I}_{t} = f \left( \boldsymbol{W}_{xi} \boldsymbol{X}_{t} + \boldsymbol{W}_{hi} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{i} \right)
\end{equation}
\begin{equation}
    \label{eq:lstm_forget}
    \boldsymbol{F}_{t} = f \left( \boldsymbol{W}_{xf} \boldsymbol{X}_{t} + \boldsymbol{W}_{hf} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{f} \right)
\end{equation}
\begin{equation}
    \label{eq:lstm_output}
    \boldsymbol{O}_{t} = f \left( \boldsymbol{W}_{xo} \boldsymbol{X}_{t} + \boldsymbol{W}_{ho} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{o} \right)
\end{equation}

As portas são calculadas pelas equações \ref{eq:lstm_input}, \ref{eq:lstm_forget} e \ref{eq:lstm_output}, respectivamente, em que $\boldsymbol{W}_{xi}$, $\boldsymbol{W}_{hi}$, $\boldsymbol{W}_{xf}$, $\boldsymbol{W}_{hf}$, $\boldsymbol{W}_{xo}$ e $\boldsymbol{W}_{ho}$ são matrizes de pesos e $\boldsymbol{b}_{i}$, $\boldsymbol{b}_{f}$ e $\boldsymbol{b}_{o}$ são vetores de viés. A função de ativação $f$ utilizada nas portas é a sigmoide, dada pela equação \ref{eq:sigmoid}, que retorna valores entre 0 e 1. Ela é utilizada para controlar a quantidade de informação que deve ser passada para a célula de memória.

\begin{equation}
    \label{eq:sigmoid}
    f(x) = \frac{1}{1 + e^{-x}}
\end{equation}

Conforme pontuado em \cite{schmidt2019recurrent}, as LSTMs têm, ainda, um componente de memória $\boldsymbol{\tilde{C}}_{t}$, cuja atualização é controlada pela combinação com as portas da célula. O principal objetivo desse elemento é regular a quantia de informação que deve ser passada para um novo estado de memória $\boldsymbol{C}_{t}$ com base no estado anterior $\boldsymbol{C}_{t-1}$. O cálculo de $\boldsymbol{\tilde{C}}_{t}$ é dado pela equação \ref{eq:lstm_ctilde}:

\begin{equation}
    \label{eq:lstm_ctilde}
    \boldsymbol{\tilde{C}}_{t} = \tanh \left( \boldsymbol{W}_{xc} \boldsymbol{X}_{t} + \boldsymbol{W}_{hc} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{c} \right)
\end{equation}

em que $\boldsymbol{W}_{xc}$ e $\boldsymbol{W}_{hc}$ são matrizes de pesos e $\boldsymbol{b}_{c}$ é um vetor de viés. A função de ativação de tangente hiperbólica é utilizada para normalizar os valores da célula de memória, retornando valores entre -1 e 1. Por fim, o estado interno da célula de memória $\boldsymbol{C}_{t}$ é calculado pela expressão \ref{eq:lstm_c} e a saída da unidade $\boldsymbol{H}_{t}$ é dada pela equação \ref{eq:lstm_h}, em que $\odot$ é a multiplicação elemento a elemento, chamada de produto de Hadamard.

\begin{equation}
    \label{eq:lstm_c}
    \boldsymbol{C}_{t} = \boldsymbol{F}_{t} \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_{t} \odot \boldsymbol{\tilde{C}}_{t}
\end{equation}

\begin{equation}
    \label{eq:lstm_h}
    \boldsymbol{H}_{t} = \boldsymbol{O}_{t} \odot \tanh(\boldsymbol{C}_{t})
\end{equation}

A Figura \ref{fig:lstm_architecture} ilustra a arquitetura geral das LSTMs.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma célula de memória das redes \textit{Long Short-Term Memory} (LSTM) em um dado tempo. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:lstm_architecture}
    \input{../figuras/lstm}
\end{figure}

A capacidade de aprender dependências de longo prazo em sequências destaca as LSTMs das redes neurais recorrentes convencionais. A estrutura de memória permite que elas capturem relações entre elementos distantes em uma sequência sem que o gradiente desvaneça ou cresça excessivamente. Isso é, em particular, importante para lidar com textos extensos, como os de uma redação.

Entretanto, as LSTMs não são isentas de desvantagens. Uma delas é a complexidade computacional, já que a estrutura de memória e as portas adicionam uma quantidade considerável de parâmetros ao modelo. Somado a isso, assim como no caso das RNNs convencionais, a arquitetura sequencial dessas redes não permite um processamento paralelo, agravando o problema de eficiência no treino.

\subsubsection{\textit{Gated Recurrent Unit} (GRU)}

Outra variação de RNNs é a \textit{Gated Recurrent Unit} (GRU), que visa simplificar a arquitetura das LSTMs, mantendo uma desempenho de avaliação semelhante. A GRU possui menos parâmetros e apenas duas portas: uma de atualização e uma de reinício (\cite{cho2014learning}). Isso a torna mais eficiente em termos computacionais e, em muitos casos, tão eficaz quanto as LSTMs.

A estrutura de uma GRU possui unidades de memória controladas por duas portas principais: a de atualização $\boldsymbol{Z}_{t}$ e a de reinício $\boldsymbol{R}_{t}$. Elas regulam o fluxo de informação de maneira eficiente à medida que atuam com menos parâmetros, sem perder a propriedade das LSTMs de assimilação de relações a longo prazo.

Em geral, a porta de atualização ajuda a capturar as dependências de longo prazo, enquanto a de reinício auxilia na modelagem das dependências de curto prazo. A primeira é calculada pela equação \ref{eq:gru_update} e a segunda pela equação \ref{eq:gru_reset}:

\begin{equation}
    \label{eq:gru_update}
    \boldsymbol{Z}_{t} = f \left( \boldsymbol{W}_{xr} \boldsymbol{X}_{t} + \boldsymbol{W}_{hr} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{r} \right)
\end{equation}
\begin{equation}
    \label{eq:gru_reset}
    \boldsymbol{R}_{t} = f \left( \boldsymbol{W}_{xz} \boldsymbol{X}_{t} + \boldsymbol{W}_{hz} \boldsymbol{H}_{t-1} + \boldsymbol{b}_{z} \right)
\end{equation}

em que $\boldsymbol{W}_{xr}$, $\boldsymbol{W}_{hr}$, $\boldsymbol{W}_{xz}$ e $\boldsymbol{W}_{hz}$ são matrizes de pesos, $\boldsymbol{b}_{r}$ e $\boldsymbol{b}_{z}$ são vetores de viés e $f$ é a função de ativação sigmoide (\ref{eq:sigmoid}). Ambas as expressões são utilizadas para calcular um estado oculto intermediário $\tilde{\boldsymbol{H}}_{t}$, que é uma versão atualizada do estado anterior $\boldsymbol{H}_{t-1}$, conforme a equação \ref{eq:gru_hidden}.

\begin{equation}
    \label{eq:gru_hidden}
    \tilde{\boldsymbol{H}}_{t} = \tanh \left( \boldsymbol{W}_{xh} \boldsymbol{X}_{t} + \boldsymbol{W}_{hh} (\boldsymbol{R}_{t} \odot \boldsymbol{H}_{t-1}) + \boldsymbol{b}_{h} \right)
\end{equation}

Na expressão, $\boldsymbol{W}_{xh}$ e $\boldsymbol{W}_{hh}$ são matrizes de pesos e $\boldsymbol{b}_{h}$ é um vetor de viés. A atualização da unidade de memória $\boldsymbol{H}_{t}$ é, então, calculada pela equação \ref{eq:gru}, conforme \citet{zhang2023dive}.

\begin{equation}
    \label{eq:gru}
    \boldsymbol{H}_{t} = (1 - \boldsymbol{Z}_{t}) \odot \tilde{\boldsymbol{H}}_{t} + \boldsymbol{Z}_{t} \odot \boldsymbol{H}_{t-1}
\end{equation}

A Figura \ref{fig:gru_architecture} apresenta a arquitetura da GRU, evidenciando as portas de atualização e reinício que controlam o fluxo de informação na unidade de memória.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de uma unidade de memória de uma \textit{Gated Recurrent Unit} (GRU) em um dado tempo. Imagem adaptada de \cite{zhang2023dive}.}
    \label{fig:gru_architecture}
    \input{../figuras/gru}
\end{figure}

As GRUs oferecem uma alternativa competitiva às LSTMs em muitas aplicações, principalmente quando a eficiência no treinamento é um requisito chave, já que ambas têm um desempenho prático semelhante. Além disso, esse modelo possui uma implementação mais simples, dada a menor complexidade de parâmetros a serem operados.

No entanto, as GRUs podem ser mais limitadas em tarefas de NLP que envolvam dependências de longo prazo, já que as LSTMs têm uma estrutura de memória mais robusta capaz de assimilar adequadamente relações distantes em textos. Isso dificulta o uso desse modelo na área de avaliação automática de redações.

\section{Modelos de Linguagem Baseados em Transformadores}

Os modelos de linguagem baseados em transformadores representam uma evolução significativa no campo de NLP. Introduzidos por \citet{attention2017}, foram motivados principalmente pela tarefa de tradução automática de textos, a fim de superar as limitações apresentadas pelas estruturas \textit{encoder-decoder} recorrentes e convolucionais.

A arquitetura dos transformadores é baseada no mecanismo de atenção, que permite ao modelo atribuir diferentes pesos a partes da entrada, codificando a importância de elementos da sequência. Essa abordagem, em especial, possibilita a modelagem de dependências entre a entrada e a saída, flexibilizando a necessidade de recorrência.

Os chamados \textit{transformers} têm se destacado devido a sua eficácia em lidar com tarefas complexas de NLP, como tradução automática, resumo de textos e geração de respostas a perguntas. Além disso, eles são particularmente adequados para lidar com textos extensos, como os de uma redação, ao passo que permitem um processamento paralelo eficiente.

\subsection{Atenção}

A atenção é o cerne dos modelos de transformadores. Esse mecanismo permite assinalar diferentes importâncias aos \textit{tokens} de entrada ao realizar uma tarefa, modelando a dependência entre eles. Seu cálculo considera uma pontuação para cada par de elementos em uma sequência, normalizada pela função \textit{softmax} para obter os pesos.

O mecanismo pode ser representado por meio de uma matriz de atenção, em que cada linha e coluna representam um elemento da sequência. Para o caso dos transformadores, que buscam modelar a relação interna entre os próprios \textit{tokens} de entrada, essa estrutura é chamada matriz de autoatenção. A figura \ref{fig:self_attention} ilustra esse conceito.

\begin{figure}[H]
    \centering
    \caption{Representação de uma matriz de autoatenção com os pesos do primeiro \textit{token} em relação a sua própria frase. Imagem inspirada em \cite{attention2017}.}
    \label{fig:self_attention}
    \begin{minipage}{0.4\textwidth}
        \centering
        \input{../figuras/self_attention_pairs}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \centering
        \input{../figuras/self_attention_matrix}
    \end{minipage}
\end{figure}

Matematicamente, dada uma sequência de entrada $\boldsymbol{X} = (x_1, x_2, ..., x_n)$ e uma função de pontuação $a$ que mede a relevância entre dois elementos, a atenção para $x_i$ em relação a um termo $k$ da sequência ($\boldsymbol{A}(x_{i}, x_{k})$) é calculada por \ref{eq:attention}:

\begin{equation}
    \label{eq:attention}
    \boldsymbol{A}(x_{i}, x_{k}) = \frac{e^{a(x_{i}, x_{k})}}{\sum_{j=1}^{n} e^{a(x_{i}, x_{j})}}
\end{equation}

em que $a$ pode ser uma função que compara os elementos, como o produto escalar, por exemplo. Esse processo é executado para todos os elementos da sequência, gerando pesos de atenção que destacam a importância relativa de cada elemento para a tarefa em questão. Os \textit{transformers} utilizam a função \textit{scaled dot-product} para modelar as relações, que será abordada e detalhada mais à frente.

\subsection{Arquitetura}

A arquitetura dos modelos de linguagem baseados em transformadores, conforme proposta por \citet{attention2017}, é caracterizada pela presença de dois módulos principais: o de codificação e o de decodificação. Ambos os componentes são organizados em camadas empilhadas, compreendendo uma estrutura que tem se destacado em tarefas variadas no âmbito do NLP.

No nível de cada camada, a arquitetura incorpora módulos de atenção \textit{multihead}, os quais desempenham um papel fundamental na modelagem de relações de dependência em diferentes partes do contexto. Esses módulos possibilitam a consideração simultânea de informações distantes, promovendo a captura eficiente de padrões sequenciais complexos. Adicionalmente, em cada camada são empregadas redes tradicionais (\textit{feed-forward}) totalmente conectadas, enriquecendo a representação ao permitir a expressão de relações não lineares entre os elementos da sequência.

Além desses elementos, as conexões residuais são introduzidas em cada camada para abordar questões relacionadas à propagação do gradiente. Essas conexões, ao fornecer atalhos na passagem do gradiente, facilitam a otimização em profundidade, promovendo, assim, a eficácia do treinamento em arquiteturas mais extensas.

Após as conexões residuais, são feitas normalizações em cada camada, cujo objetivo é a manutenção da estabilidade e a facilitação da convergência durante o treinamento (\cite{zhang2023dive}). Tal prática contribui para a eficácia do aprendizado ao mitigar possíveis desafios associados à variação na escala dos dados em arquiteturas profundas, como no caso dos \textit{transformers}.

Como os transformadores não possuem uma estrutura sequencial inerente, o modelo incorpora uma etapa de codificação posicional, essencial para fornecer informações sobre a ordem das palavras em uma sequência de entrada. Essa codificação pode ser, ainda, pré-determinada ou aprendida pela rede. Ao adicionar dados de posição às representações vetoriais, a técnica capacita o mecanismo de atenção a antecipar a ordem temporal das palavras, permitindo um melhor desempenho dos \textit{transformers} em contextos linguísticos complexos.

Em suma, os modelos baseados em transformadores apresentam uma configuração estratificada que integra diversos componentes para realizar operações de atenção, conectividade total e normalização, culminando em uma abordagem poderosa para lidar, em especial, com linguagens. A figura \ref{fig:transformer_architecture} ilustra o diagrama de sua arquitetura, em que $N$ é um número arbitrário pré-definido de camadas empilhadas.

\begin{figure}[H]
    \centering
    \caption{Arquitetura de um modelo de linguagem baseado em transformadores. Imagem extraída de \cite{attention2017}.}
    \label{fig:transformer_architecture}
    \input{../figuras/transformer}
\end{figure}

Na estrutura da figura \ref{fig:transformer_architecture}, à esquerda, estão representados os módulos de codificação, que geralmente são utilizados para gerar representações matemáticas da entrada que capturem relações semânticas. Já à direita, estão os módulos de decodificação, mais focados em tarefas de previsão de \textit{tokens} em frases, geração de textos e elaboração de respostas a perguntas.

O foco deste trabalho é a tarefa de avaliação automática de redações, que envolve, principalmente, a etapa de codificação das produções. A seguir, serão detalhados os mecanismos de atenção multicabeça e as subestruturas dos transformadores.

\subsubsection{\textit{Multihead Attention}}

Em geral, o mecanismo de atenção, conforme \citet{zhang2023dive}, pode ser representado por meio de pares de chave-valor $(k, v)$ e de consultadores $q$, em que podemos utilizar $q$ para obter informações de uma base de dados de diferentes $(k, v)$. Seu cálculo, nesse caso, é dado de acordo com a equação \ref{eq:attention_key_value_query}, em que $\mathcal{D}$ é o conjunto de pares chave-valor ($\mathcal{D} = \{(k_{1}, v_{1}), \cdots, (k_{m}, v_{m})\}$).

\begin{equation}
    \label{eq:attention_key_value_query}
    \boldsymbol{A}(\mathcal{D}, q) = \sum_{i=1}^{m} \boldsymbol{\alpha}(q, k_{i}) v_{i}
\end{equation}

A atenção \textit{multihead} é uma extensão desse mecanismo, que permite que o modelo aprenda diferentes tipos de relações entre os elementos da sequência, ao mesmo tempo em que reduz a complexidade computacional ao considerar uma abordagem paralelizável com matrizes de consultadores ($Q$), chaves ($K$) e valores ($V$) (\cite{attention2017}).

Os pesos, nessa técnica, são calculados por meio de uma projeção linear da entrada, seguida da função de atenção \textit{scaled dot-product} abaixo e de uma projeção linear da saída. O cálculo dessa função utiliza uma escala inversamente proporcional à dimensão dos consultadores e chaves $d_{k}$ (\ref{eq:scaled_dot_product}):

\begin{equation}
    \label{eq:scaled_dot_product}
    \boldsymbol{\alpha}(Q, K) = \text{softmax} \left( \frac{QK^{T}}{\sqrt{d_{k}}} \right)
\end{equation}

O valor da atenção resultante é, então, dado pela equação \ref{eq:multihead_attention}.

\begin{equation}
    \label{eq:multihead_attention}
    \boldsymbol{A}(Q, K, V) = \boldsymbol{\alpha}(Q, K)V
\end{equation}

Ao fim, o resultado de cada cabeça é concatenado e novamente projetado linearmente para formar a representação final $\boldsymbol{M}_{a}$, cujo cálculo é dado pela equação \ref{eq:multihead_weights}:

\begin{equation}
    \label{eq:multihead_weights}
    \boldsymbol{M}_{a} = \text{concat}(\boldsymbol{H}_{1}, \cdots, \boldsymbol{H}_{h})W^{O}
\end{equation}

em que $\boldsymbol{H}_{i}$, para $i \in \{1, \cdots, h\}$, é a saída da $i$-ésima cabeça de atenção e $W^{O}$ é uma matriz de pesos. $\boldsymbol{H}_{i}$ é dado, em função de $\boldsymbol{A}$, pela expressão \ref{eq:head_by_attn}.

\begin{equation}
    \label{eq:head_by_attn}
    \boldsymbol{H}_{i} = \boldsymbol{A}(\boldsymbol{Q}_{i}W_{i}^{Q}, \boldsymbol{K}_{i}W_{i}^{K}, \boldsymbol{V}_{i}W_{i}^{V})
\end{equation}

Em que $W_{i}^{Q}$, $W_{i}^{K}$ e $W_{i}^{V}$ são matrizes de pesos e $\boldsymbol{Q}_{i}$, $\boldsymbol{K}_{i}$ e $\boldsymbol{V}_{i}$ são vetores de consultadores, chaves e valores da $i$-ésima cabeça, respectivamente. A figura \ref{fig:multihead_attention} ilustra o cálculo da atenção \textit{multihead}.

\begin{figure}[H]
    \centering
    \caption{À direita, o cálculo da atenção \textit{multihead}. À esquerda, a função de atenção \textit{scaled dot-product}. Imagem adaptada de \cite{attention2017}.}
    \label{fig:multihead_attention}
    \begin{minipage}{0.35\textwidth}
        \centering
        \input{../figuras/scaled-dot_product}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \input{../figuras/multihead_attention}
    \end{minipage}
\end{figure}

\subsubsection{Codificador}

O codificador processa a entrada sequencial e produz uma representação contextualizada para cada elemento. Cada camada desse módulo contém duas subcamadas subsequentes: a de atenção \textit{multihead} e a de redes neurais totalmente conectadas.

A subcamada de atenção \textit{multihead} permite que o modelo aprenda diferentes tipos de relações entre os elementos da sequência já codificados. Cada cabeça opera em uma projeção linear da entrada, e as saídas são concatenadas e novamente projetadas linearmente para formar a representação final $\boldsymbol{X}$, após a normalização e conexão residual de $\boldsymbol{M}_{a}$, conforme mostrado na seção anterior.

Em seguida, a saída é submetida a uma rede neural totalmente conectada, que consiste em duas camadas lineares com uma função de ativação ReLU (indicada pela equação \ref{eq:relu}) entre elas. Essa subcamada permite que o modelo aprenda representações não lineares dos dados, enriquecendo a representação final.

\begin{equation}
    \label{eq:relu}
    f(x) = \max(0, x)
\end{equation}

A saída de uma camada do codificador, então, é dada pela normalização e conexão residual da equação \ref{eq:encoder_layer}:

\begin{equation}
    \label{eq:encoder_layer}
    \boldsymbol{S} = f(\boldsymbol{W_{1}} \boldsymbol{X} + \boldsymbol{b}_{1})\boldsymbol{W_{2}} + \boldsymbol{b}_{2}
\end{equation}

em que $\boldsymbol{W_{1}}$, $\boldsymbol{W_{2}}$, $\boldsymbol{b}_{1}$ e $\boldsymbol{b}_{2}$ são parâmetros da rede neural totalmente conectada (matrizes de pesos e vetores de vieses, nessa ordem) e $f$ é a função de ativação ReLU (\ref{eq:relu}).

\subsubsection{Decodificador}

O decodificador gera uma sequência de saída contextualizada com base na representação do codificador. Assim como no módulo de codificação, cada camada dessa etapa também possui subcamadas de atenção \textit{multihead} e redes neurais totalmente conectadas. O que difere o módulo decodificador, entretanto, é a inclusão de uma nova camada de atenção com máscara antes da entrada das representações codificadas.

A máscara de atenção é utilizada para evitar que o modelo tenha acesso a informações futuras durante a geração da sequência de saída. Ela faz com que o mecanismo de atenção atue nas codificações dos elementos de modo a suprimir antecipações temporais da sequência pelos \textit{transformers}.

O cálculo final de uma camada do codificador é dado de maneira similar ao da seção anterior, com a inclusão das codificações de entrada e da nova etapa de atenção com máscara.

\section{BERT: \textit{Bidirectional Encoder Representations from Transformers}}

Os modelos baseados em transformadores representaram um marco significativo no avanço da área do NLP, redefinindo as fronteiras da compreensão de linguagem por máquinas. Entre as inovações motivadas pelo surgimento dos \textit{transformers}, destaca-se o BERT (\textit{Bidirectional Encoder Representations from Transformers}), um modelo introduzido por \citet{bert2018} que revolucionou o processamento de sequências textuais pelo computador.

O BERT se diferencia ao capturar, de maneira única, o contexto bidirecional das palavras em uma sequência, superando as limitações dos modelos unidirecionais. Sua arquitetura, baseada em múltiplas camadas de transformadores empilhadas, permite uma codificação contextual robusta dos elementos, capturando relações não só com as palavras anteriores, mas também subsequentes.

O processo de pré-treinamento do BERT permite ao modelo compreender contextos de forma sofisticada, promovendo uma representação rica de palavras ou \textit{tokens} em textos. Além disso, a arquitetura pode ser utilizada como base para o treinamento de sistemas especialistas por meio, geralmente, do aprendizado supervisionado. Neste trabalho, por exemplo, o BERT é refinado para atuar como um classificador de textos, com o objetivo de avaliar redações no modelo do ENEM.

A fim de se adequar à diversidade linguística, foram desenvolvidas algumas variações do modelo a partir de seu pré-treinamento em conjuntos de textos específicos de uma dada linguagem. Dentre as versões, destaca-se o BERTimbau, uma variante do BERT com foco no português brasileiro (\cite{bertimbau2020}). As diferentes alternativas existentes demonstram a capacidade de generalização do modelo, que é capaz de aprender particularidades semânticas e sintáticas de diversas línguas.

O BERT representa não apenas um avanço técnico, mas também uma ferramenta essencial para diversas aplicações em NLP, incluindo a tarefa de AES. A seguir, abordaremos brevemente os aspectos de sua arquitetura, o processo de treinamento e refinamento, a variação do modelo para a língua portuguesa e, por fim, como o BERT pode ser usado para atribuir notas às redações do ENEM de forma automática.

\subsection{Arquitetura}

A arquitetura do BERT é meticulosamente projetada para capturar de forma eficaz as relações contextuais em uma sequência de palavras, incorporando transformadores bidirecionais em múltiplas camadas. Cada camada desempenha um papel crucial na codificação contextual, permitindo que o modelo compreenda as dependências tanto à esquerda quanto à direita de cada unidade. A Figura \ref{fig:bert_architecture} esquematiza a arquitetura geral do BERT.

\begin{figure}[H]
    \centering
    \caption{Arquitetura geral do BERT, considerando a codificação dos \textit{tokens} e as camadas de transformadores. Imagem adaptada de \cite{cui-etal-2022-deep}}.
    \label{fig:bert_architecture}
    \input{../figuras/bert_architecture}
\end{figure}

Antes de serem processadas pelas camadas de transformadores, as sequências de palavras de entrada são codificadas em \textit{tokens}. A tokenização divide as palavras em unidades menores, permitindo que o modelo lide de maneira mais flexível com diferentes formas e variações linguísticas. A tokenização em subpalavras também ajuda no desafio de lidar com o vocabulário diversificado e a variabilidade morfológica presente em diferentes idiomas.

Alguns \textit{tokens} especiais também são levados em conta na modelagem. O \textit{token} \texttt{[CLS]}, por exemplo, é adicionado ao início de cada sequência e representa a tarefa de classificação da sentença. Já o \textit{token} \texttt{[SEP]} é adicionado ao final de cada sentença, indicando seu fim. Esses elementos especiais são importantes, em particular, na etapa de treinamento, já que tanto pares de pergunta e resposta quanto textos completos a serem classificados podem ser modelados sem mudar o formato da sequência de entrada (\cite{bert2018}).

O BERT mantém, além disso, vetores que codificam a posição e o segmento de cada \textit{token} na sequência, permitindo que o modelo entenda a ordem de palavras e o mecanismo de separação de sentenças. Essas codificações são essenciais para a compreensão de textos, já que a posição das palavras é relevante para o significado.

As camadas de transformadores, então, recebem como entrada os vetores de \textit{tokens}, posição e segmento, codificando relações contextuais entre as palavras. Cada camada, por meio das conexões bidirecionais entre todos os \textit{transformers} anteriores, gera uma representação dos vocábulos de entrada. Isso possibilita ao BERT capturar interações complexas entre os elementos, fornecendo uma compreensão aprimorada de uma sequência. As saídas de todas as camadas são combinadas para formar uma representação final enriquecida.

Os autores \citet{bert2018} propõem duas arquiteturas do BERT: uma basilar ($\text{\textbf{BERT}}_{\text{\textbf{BASE}}}$) de 12 camadas de transformadores empilhadas, com 12 cabeças de atenção em cada uma, e outra estendida ($\text{\textbf{BERT}}_{\text{\textbf{LARGE}}}$) de 24 camadas de transformadores, com 16 cabeças de atenção em cada uma.

% continuar daqui SEM se matar

\subsection{Pré-treinamento e \textit{Fine-tuning}}

O pré-treinamento é a etapa em que um modelo de linguagem é capaz de aprender aspectos semânticos e contextuais das sequências, a partir de um grande conjunto de dados. Para cumprir esse objetivo, o BERT é submetido a duas tarefas: a predição de palavras mascaradas e a identificação de relação entre duas sentenças (\cite{zhang2023dive}).

A primeira tarefa, chamada \textit{Masked Language Model} (MLM), consiste em mascarar aleatoriamente 15\% dos \textit{tokens} de entrada e treinar o modelo para prever as palavras mascaradas com base no contexto global.  Essa abordagem promove a compreensão bidirecional de uma sequência, já que as relações de dependência interna de um texto são assimiladas à medida que o BERT otimiza seus parâmetros para cumprir tal função.

Já a segunda tarefa, chamada \textit{Next Sentence Prediction} (NSP), envolve a apresentação de pares de sentenças ao modelo, que deve prever se uma ocorre após a outra no texto original. Essa atribuição aprimora a capacidade do BERT de entender relações semânticas, sendo especialmente útil em contextos de resposta a perguntas ou realização de inferências lógicas.

O pré-treinamento do BERT, em \cite{bert2018}, foi realizado a partir de um conjunto de dados de 3,3 bilhões de palavras, das quais 2,5 bilhões são de artigos do Wikipédia em inglês e 800 milhões são do BookCorpus, um compilado de cerca de 7000 livros autopublicados (\cite{zhu2015aligning}). Além disso, para a tokenização das palavras, foram utilizadas as codificações do algoritmo WordPiece (\cite{wu2016googles}), com um vocabulário de 30000 \textit{tokens}.

Após o pré-treinamento, o BERT pode ser ajustado para tarefas específicas por meio de um processo chamado \textit{fine-tuning}. Durante essa etapa, o modelo pode ser acoplado a outras arquiteturas, que são treinadas em conjuntos de dados rotulados de acordo com a tarefa desejada. Uma mesma versão pré-treinada do BERT pode ser aplicada a diferentes tarefas de NLP, uma vez que o modelo pré-treinado já possui uma compreensão sofisticada da linguagem, o que caracteriza essa abordagem, também, como menos computacionalmente custosa.

A figura \ref{fig:bert_training_tuning} ilustra o processo de pré-treinamento e \textit{fine-tuning} do BERT.

\begin{figure}[H]
    \centering
    \caption{Processo de pré-treinamento do BERT, à esquerda, e uso de modelos pré-treinados no \textit{fine-tuning}, à direita. Imagem extraída de \cite{bert2018}.}
    \label{fig:bert_training_tuning}
    \includegraphics[width=\textwidth]{../figuras/bert_training_tuning}
\end{figure}

\subsection{BERTimbau: BERT para o Português Brasileiro}

A demanda por capturar nuances complexas da língua portuguesa impulsionou o desenvolvimento do BERTimbau, uma adaptação do BERT para o português brasileiro (\cite{bertimbau2020}). Sem alterar a arquitetura ou o processo de pré-treinamento, essa versão usa uma base de textos distinta para ajustar os pesos do modelo, aprendendo a lidar com o vocabulário nacional.

O BERTimbau foi treinado utilizando o conjunto de dados brWaC (\cite{filho-etal-2018-brwac}), uma extensa coleção de textos em português extraídos e filtrados de páginas da web. A base é formada por 2,7 bilhões de \textit{tokens}, que foram reprocessados por \citet{bertimbau2020} para a retirada de \textit{mojibakes}\footnote{Os \textit{mojibakes} são anomalias que acometem textos da internet devido a erros de interpretação de caracteres em diferentes codificações. A exemplo, a palavra "redação", na codificação UTF-8, será mostrada como "redaÃ§Ã£o", na codificação ISO-8859-1. Esses problemas são, em particular, comuns na língua portuguesa devido à riqueza em acentuação.} e \textit{tags} de marcação reminescentes.

A tokenização do conjunto de dados, crucial para lidar com a morfologia variada do português, foi realizada pelo algoritmo SentencePiece (\cite{kudo2018sentencepiece}), com posterior conversão para o formato do WordPiece a fim de manter consistência com os \textit{tokens} do BERT original.

O processo de treinamento do BERTimbau foi realizado com as mesmas duas tarefas de \citet{bert2018}, a partir da predição de palavras ocultas em sentenças e da identificação de relação entre duas frases. Essas etapas contribuem, em especial, para a capacidade do modelo em compreender complexidades semânticas do português, aprimorando sua eficácia ao lidar com textos nessa língua. O BERTimbau também está disponível nas variantes basilar e extensa, ambas com a mesma quantidade de parâmetros das versões do BERT original, o que permite adaptações de acordo com a necessidade da tarefa final a ser realizada.

A disponibilidade de um modelo pré-treinado em língua portuguesa proporciona uma base sólida para uma gama de aplicações em contexto nacional, incluindo a avaliação automática de redações no formato do ENEM, explorada nesta monografia.

\subsection{O BERTimbau na Avaliação Automática de Redações do ENEM}

Devido a sua capacidade de capturar relações complexas em textos da língua portuguesa, o BERTimbau pode ser utilizado na tarefa de AES no âmbito do ENEM. Com o modelo, que é responsável por codificar as redações em \textit{embeddings}, é possível obter representações matemáticas ricas em significado, permitindo a utilização de técnicas de aprendizado de máquina capazes de se especializar em atribuir notas aos textos.

Destaca-se, também, que modelos de linguagem como o BERTimbau, que se beneficiam da paralelização, podem ser utilizados para avaliar redações em larga escala de forma relativamente barata, evitando os gargalos logísticos decorrentes de uma correção exclusivamente humana.

Algumas das técnicas de treinamento de sistemas especializados são os algoritmos supervisionados, que podem ser otimizados para seguir um padrão de um conjunto de dados rotulados. O BERTimbau, por exemplo, pode ser acoplado a uma rede neural, treinada para classificar redações de acordo com notas atribuídas por corretores humanos. Tal é a abordagem deste trabalho, que será melhor explorada no capítulo \ref{chap:work}.
