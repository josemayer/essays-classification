% TODO: Adicionar referências ao longo de toda a introdução

\chapter{Introdução}

Tradicionalmente, técnicas de classificação de texto têm desempenhado um papel fundamental na avaliação e atribuição de notas a redações com base em vários parâmetros, como conteúdo, estrutura, coerência e proficiência linguística. Algoritmos clássicos de aprendizado de máquina têm sido utilizados para identificar e avaliar características-chave dentro das redações, com o intuito de mimetizar o processo humano de correção manual dos textos. Essas técnicas muitas vezes dependem de padrões avaliados sob medida e de conhecimento específico do domínio, o que pode ser trabalhoso e menos adaptável à natureza dinâmica da linguagem.

Além disso, os modelos tradicionais de avaliação textual automática enfrentam desafios na compreensão da semântica e do contexto das redações. Por exemplo, podem ter dificuldade em reconhecer nuances, metáforas ou referências culturais, que são aspectos críticos de uma correção eficaz. Tal limitação abriu caminho para modelos mais sofisticados, capazes de capturar a essência da linguagem humana de maneira mais abrangente.

Em particular, o surgimento recente de modelos de linguagem, como as arquiteturas de \textit{transformers} (\cite{attention2017}) e o \textit{Bidirectional Encoder Representations from Transformers} (BERT) (\cite{bert2018}), auxiliou na mitigação desse problema. Os transformadores se destacaram em várias tarefas de Processamento de Linguagem Natural (NLP) ao considerar o contexto e as relações entre palavras, tornando-os excepcionalmente adequados para a avaliação de redações. O BERT, que, por sua vez, é inspirado na lógica dos \textit{transformers}, revolucionou a área de NLP ao introduzir os \textit{embeddings}\footnote{\textit{Embeddings}, no âmbito do NLP, são representações vetoriais numéricas de palavras, cuja codificação tem o intuito de garantir que o computador entenda seu contexto e semântica.} contextualizados de palavras.

Modelos como o BERT operam com base nos princípios de pré-treinamento e de ajuste fino. Durante a fase do pré-treinamento, aprendem as complexidades da linguagem ao prever palavras ausentes em frases e compreender o contexto em que são usadas. Posteriormente, o ajuste fino permite que os modelos se adaptem a tarefas específicas, como a correção automática, a partir do treinamento com dados já rotulados. A capacidade de considerar a redação como uma unidade contextual, em oposição a palavras ou frases isoladas, capacita as arquiteturas baseadas em \textit{transformers} a capturar as complexidades da linguagem, incluindo semântica, coerência e até mesmo significados subjacentes ao texto.

Tais qualidades desempenham um papel fundamental na correção automática de testes de conhecimento e produção textual, como no caso do Exame Nacional do Ensino Médio (ENEM), um dos principais instrumentos de avaliação educacional do Brasil. Na prova de redação desse exame, são estabelecidos critérios objetivos de pontuação que abordam competências como coesão textual, argumentação e domínio temático, características que podem ser assimiladas na etapa de pré-treinamento, por exemplo.

Utilizando-se do BERT e dos modelos de linguagem baseados na arquitetura de \textit{transformers}, os sistemas de avaliação automática de redações testemunham um avanço significativo na capacidade de imitar os avaliadores humanos. Esses modelos podem avaliar a qualidade completa do texto, reconhecer nuances da língua portuguesa e fornecer uma nota coerente de acordo com critérios objetivos de correção. Ao longo desta monografia, exploraremos os princípios do ajuste fino do BERT na correção automática de redações com enfoque no modelo do ENEM, os desafios abarcados por essa tarefa e, por fim, o potencial dessa abordagem em relação aos métodos tradicionais e humanos de avaliação.

\section{Contextualização}

A avaliação de redações em contextos educacionais é uma tarefa de grande importância, pois permite mensurar a capacidade dos estudantes de comunicar suas ideias de maneira clara, coerente e persuasiva. No entanto, no cenário de acesso a esses textos em língua portuguesa, especialmente no âmbito do ENEM no Brasil, a disponibilidade de conjuntos de dados rotulados tem sido historicamente limitada, como pontuado em \cite{marinho-et-al-21}. Apesar do desenvolvimento de diversos sistemas de avaliação automática de redações em língua inglesa, adaptados para exames como o \textit{Test of English as a Foreign Language} (TOEFL), poucos surgiram para atender a textos em português ou ao modelo específico exigido pelo ENEM.

Dentre os poucos estudos na área, desafios adicionais têm sido a utilização histórica de mecanismos mais primordiais de aprendizado de máquina e a falta de transparência sobre os dados. Trabalhos anteriores nesse campo, como em \cite{amorim-et-al-2018}, utilizaram técnicas de extração de características dos textos e métodos mais simples para pontuar as redações. Além disso, as bases de textos utilizadas muitas vezes não são divulgadas publicamente, como no estudo de \cite{fonseca-et-al-2018}, dificultando a verificação e replicação dos resultados e limitando a aplicação de abordagens mais sofisticadas.

Com o intuito de enfrentar essas barreiras, em \cite{marinho-et-al-21}, os autores desenvolveram uma base de redações composta por 4570 textos dissertativo-argumentativos escritos por estudantes do ensino médio no Brasil e avaliados por especialistas na área de correção desses textos, nomeada Essay-BR. Tal conjunto foi projetado especificamente para atender às necessidades da avaliação automática de redações no modelo do ENEM, dado que as notas foram atribuídas de acordo com a matriz de referência do exame.

Para criar o conjunto das redações, os autores utilizaram técnicas de raspagem de dados por meio de ferramentas de extração e de filtragem, que permitiram a coleta dos textos e avaliações, organizados entre dezembro de 2015 e abril de 2020, com base em dois sites públicos: \href{https://vestibular.brasilescola.uol.com.br/banco-de-redacoes}{Vestibular UOL} e \href{https://educacao.uol.com.br/bancoderedacoes}{Educação UOL}. Ambos os portais educacionais são fontes confiáveis, uma vez que os ensaios foram escritos por estudantes do ensino médio e avaliados por especialistas, seguindo os critérios do ENEM.

A diversidade de temas presentes no Essay-BR também é uma característica notável. Com um total de 86 tópicos-base, o conjunto de dados reflete a ampla gama de proposições abordadas no ENEM, abrangendo questões relacionadas a direitos humanos, desafios políticos, movimentos populares, dentre outros. Isso torna a base de textos uma representação fiel das complexidades e diversidades temáticas enfrentadas pelos estudantes no exame, o que é fundamental para avaliações mais precisas.

O conjunto de dados possui, também, uma versão estendida contendo 6479 textos e 151 temas. Essa versão foi coletada entre o período de dezembro de 2015 a agosto de 2021, com a mesma técnica utilizada para a coleta do Essay-BR (\cite{marinho-et-al-22}). As redações da base estendida também têm notas atribuídas de acordo com a matriz de referência do ENEM, variando somente na formatação da estrutura de colunas.

A disponibilidade pública desses dados promove, por fim, a transparência e a replicabilidade da pesquisa nesse campo, possibilitando que pesquisadores apliquem abordagens mais sofisticadas para avanços na correção automática de redações. Essas bases desempenham um papel crucial no desenvolvimento de sistemas de avaliação automática em língua portuguesa e são as principais referências utilizadas por este trabalho.

\section{Motivação}

O Exame Nacional do Ensino Médio (ENEM) é uma avaliação educacional de grande importância no Brasil, realizada anualmente pelo Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP). Desde sua criação em 1998, o exame tem evoluído para se tornar uma referência no processo de acesso ao ensino superior no país. A pontuação obtida pelos estudantes no ENEM é utilizada para diversos fins, sendo um dos mais notáveis o ingresso em universidades públicas.

A redação é uma das principais provas do exame, cujo intuito é avaliar a defesa de um ponto de vista com respeito a um tema definido \textit{a priori}. Ela é capaz de mensurar a capacidade dos estudantes de comunicar ideias de forma clara, coerente e persuasiva, respeitando o gênero dissertativo-argumentativo e a norma padrão da língua portuguesa. Além disso, as habilidades críticas, como a análise de informações a construção de argumentos sólidos, também são levadas em conta no processo de correção da prova (\cite{cartilha-redacao}).

A avaliação das redações é baseada em cinco competências, cujas notas podem variar de 0 a 200 em intervalos de 40 pontos. A pontuação final é calculada somando-se todos esses valores, de modo que o resultado final esteja entre 0 e 1000. O processo de correção de cada redação é feito por dois avaliadores independentes e, caso as notas divirjam em mais de 100 pontos no total ou em mais de 80 pontos em cada competência, o texto é corrigido por um terceiro profissional. Por fim, se a pontuação ainda for divergente com ambas as avaliações, a redação é avaliada por uma banca de três professores.

Entretanto, a correção manual de uma grande quantidade de redações representa um desafio logístico significativo, levando algumas organizações que atuam nessa área a, inclusive, adotar soluções mais ágeis, como apontado por \cite{taghipour-ng-2016-neural}. Ademais, a disparidade entre a quantidade de profissionais e o número de estudantes que participam do ENEM a cada ano leva a uma sobrecarga que, além de aumentar o tempo de espera dos resultados, exige um esforço excessivo por parte dos avaliadores (\cite{lesme-20-redacoes-noticia}).

Nessa perspectiva, a automação da correção de redações utilizando modelos de linguagem pode configurar uma solução promissora para o problema. O BERT e outras arquiteturas baseadas em \textit{transformers} têm o potencial de oferecer uma avaliação objetiva das redações dos estudantes, dado que eles podem ser refinados com uma gama de exemplos de redações já avaliadas por profissionais especializados e, assim, aprender a reconhecer critérios de qualidade para cada competência com base em dados reais.

Além disso, a avaliação automática permite uma resposta mais rápida e eficiente, uma vez que as notas podem ser geradas em um período mais curto. Isso facilita o processo logístico de divulgação dos resultados, reduzindo significativamente o tempo necessário para que os estudantes tenham acesso às suas pontuações.

\section{Objetivos}

Os principais objetivos deste trabalho são:

\begin{itemize}
    \item Desenvolver um modelo de correção automática de redações baseado no BERT, utilizando a arquitetura de \textit{transformers} para avaliar e pontuar redações em língua portuguesa.
    \item Comparar o desempenho do modelo desenvolvido com métodos tradicionais de extração de características de texto e pontuação de redações.
    \item Avaliar a acurácia e as métricas de avaliação do modelo desenvolvido em relação às notas atribuídas pelos avaliadores humanos nas redações, considerando as cinco competências estabelecidas no ENEM.
    \item Analisar os resultados obtidos e discutir as vantagens e desvantagens da abordagem proposta em relação aos métodos tradicionais e à correção manual realizada por avaliadores humanos.
    \item Documentar o processo de escolha da arquitetura do modelo, o treinamento com \textit{tuning} de hiperparâmetros, as dificuldades encontradas ao longo do desenvolvimento e os resultados alcançados.
\end{itemize}

Ao longo da monografia, cada tópico será melhor desenvolvido e detalhado, de modo a fornecer uma visão mais concreta sobre o escopo do trabalho e os resultados obtidos.
