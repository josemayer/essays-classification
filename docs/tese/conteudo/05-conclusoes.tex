\chapter{Conclusões}

Neste trabalho, foram explorados o desenvolvimento e a avaliação de modelos de correção automática de redações em língua portuguesa, com foco no contexto do ENEM no Brasil. Para isso, foi utilizada a base de dados Essay-BR, que contém diversos textos avaliados por profissionais, na construção de sistemas especializados em atribuir notas a cada competência exigida pelo exame.

A abordagem adotada utilizou o BERT, uma arquitetura baseada em \textit{transformers}, para extrair informações relevantes dos textos dissertativo-argumentativos produzidos pelos alunos, com o propósito de treinar modelos generalistas o suficiente para pontuar novas produções.

A avaliação dos sistemas produzidos também foi realizada com base no conjunto de dados da Essay-BR, considerando instâncias isoladas de teste. Nesse caso, diversas técnicas de treino, como o \textit{hypertuning}, foram empregadas, com o intuito de avaliar a eficácia real da construção desses modelos e responder se, de fato, são viáveis de aplicação frente à correção humana.

Durante o trabalho, um dos desafios enfrentados foi o antagonismo entre o tempo de treino e a convergência das redes no processo de \textit{hypertuning}. Devido ao grande espaço de exploração de hiperparâmetros, tornou-se necessário diminuir o número de épocas no treinamento de cada configuração, o que levou a escolhas genéricas de valores que não necessariamente refletiam o cenário de convergência do processo, mas eram mais sensíveis às inicializações aleatórias das redes. A forma plausível de se solucionar tal problema seria pelo aumento do número de épocas, que geraria, no entanto, períodos de treino mais longos.

Os resultados obtidos, contemplados no capítulo \ref{chap:experiments}, corroboraram essas dificuldades. Eles mostram que a técnica de \textit{hypertuning} não produz efeitos satisfatórios em relação ao aprendizado dos sistemas, já que, a partir dela, são escolhidos como melhores hiperparâmetros os valores que só otimizam as redes na média, mas não as generalizam. Além disso, dadas as frequentes oscilações da função de perda ao longo dos treinos com as melhores configurações, evidenciadas pelos gráficos \ref{fig:exp-hyp-c1}, \ref{fig:exp-hyp-c2}, \ref{fig:exp-hyp-c3}, \ref{fig:exp-hyp-c4} e \ref{fig:exp-hyp-c5}, nota-se que a quantidade de dados pode não ser suficiente para que essas redes aprendam adequadamente a tarefa de avaliação.

Por outro lado, a técnica de \textit{fine-tuning} com hiperparâmetros fixados, que foi utilizada para treinar os melhores modelos obtidos, mostrou-se mais eficaz, já que é possível realizar experimentos que contemplem um número maior de épocas utilizando-se de configurações com maiores chances de convergência. Apesar de ser igualmente difícil encontrar valores razoáveis iniciais, a formulação de possíveis heurísticas no método manual pode tornar o processo de escolha de novos hiperparâmetros mais rápido.

Outra constatação importante foi a da eficácia da Essay-BR básica em relação a sua versão estendida. Apesar de conter mais dados, notou-se, pela tabela \ref{tab:eval-metrics}, que a base com mais instâncias apresentou subdesempenho em relação às métricas do conjunto simplificado. Isso pode ser consequência da introdução das novas redações de uma mesma fonte à Essay-BR, que podem não ter agregado valores de generalização suficientes ao treinamento.

De modo geral, os resultados obtidos com os modelos treinados foram satisfatórios, já que os melhores sistemas foram capazes de pontuar as redações com notas próximas às atribuídas pelos avaliadores humanos. No entanto, a análise dos erros cometidos pelos modelos mostrou que ainda há muito espaço para melhorias, principalmente no que diz respeito à pontuação das competências II e V, que avaliam a adequação ao tema e a capacidade de elaborar uma proposta de intervenção para o problema, respectivamente.

Deduz-se que um dos impasses da avaliação dessas duas competências é a correlação intrínseca que ambas possuem com o tema das redações, informação não contemplada nos textos submetidos a treino. Uma das possíveis soluções seria adicionar esse dado no início da entrada de cada rede especialista, utilizando o \textit{token} especial \texttt{[SEP]} para realizar a divisão entre o tema e a redação.

Como abordagens futuras, sugere-se a construção de um mecanismo de explicabilidade dos modelos, que permita a análise de quais características dos textos são mais relevantes para a atribuição de notas. Além disso, a utilização de técnicas de \textit{data augmentation} pode ser uma alternativa para a melhoria dos resultados, já que a base de dados utilizada é relativamente pequena. Por fim, utilizar novos modelos de linguagem pode ser uma escolha viável para obter resultados melhores, permitindo uma comparação que não se limite ao nível da arquitetura das redes especialistas, mas também explore suas estruturas subjacentes.
